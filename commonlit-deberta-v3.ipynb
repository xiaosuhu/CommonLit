{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":18.376581,"end_time":"2023-08-05T11:57:47.268900","exception":false,"start_time":"2023-08-05T11:57:28.892319","status":"completed"},"tags":[]},"outputs":[],"source":["\n","import os\n","import gc\n","from tqdm.auto import tqdm\n","import transformers\n","import numpy as np \n","import pandas as pd \n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error\n","import plotly.express as px #graphing\n","import plotly.graph_objects as go #graphing\n","from plotly.subplots import make_subplots #graphing\n","import plotly.figure_factory as ff #graphing\n","from torch.nn.parameter import Parameter\n","from transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n","from transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n","\n","\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","from datasets import concatenate_datasets,load_dataset,load_from_disk\n","\n","from sklearn.metrics import log_loss\n","\n","from transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","import time\n","import warnings\n","import collections\n","# from termcolor import colored\n","\n","from torch.optim import lr_scheduler\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.016027,"end_time":"2023-08-05T11:57:47.293377","exception":false,"start_time":"2023-08-05T11:57:47.277350","status":"completed"},"tags":[]},"outputs":[],"source":["OUTPUT_DIR = './'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.015833,"end_time":"2023-08-05T11:57:47.317500","exception":false,"start_time":"2023-08-05T11:57:47.301667","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import (\n","    AutoModel, AutoConfig, \n","    AutoTokenizer, logging\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.055438,"end_time":"2023-08-05T11:57:47.381677","exception":false,"start_time":"2023-08-05T11:57:47.326239","status":"completed"},"tags":[]},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","\n","\n","class cfg:\n","    select = 'base'\n","    model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n","    only_model_name = f'deberta-v3-{select}'\n","    # model_name = f'./Models/deberta-{select}'\n","    # only_model_name = f'./Models/deberta-{select}'\n","    accum_iter = 16\n","    fold = 4\n","    split = 5\n","    seed = 42\n","    batch_size = 4\n","    max_len = 512\n","    num_epoch = 3\n","    T_max= 500\n","    \n","    scheduler = 'CosineAnnealingLR'\n","    weight_decay =  1e-6\n","    min_lr = 1e-6\n","    freezing = False\n","    pooling = 'GemText'\n","    weight_decay = 1e-2\n","    encoder_lr = 1e-5\n","    decoder_lr = 1e-5\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","\n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n","        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def score_loss(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return {\n","        'mcrmse_score' : mcrmse_score,\n","        'Content_score' : scores[0],\n","        'Wording_score' : scores[1]\n","    }\n","\n","def get_logger(filename='Training'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","\n","def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","set_seed(cfg.seed)\n","\n","\n","LOGGER.info(f\"=========================== Model name :{cfg.only_model_name} ===========================: \")\n","LOGGER.info('\\n')\n","LOGGER.info(f\"Scheduler: {cfg.scheduler}\")\n","LOGGER.info(f\"batch_size: {cfg.batch_size} with gradient Accumukation {cfg.accum_iter} \")\n","LOGGER.info(f\"Pooling name: {cfg.pooling} \")\n","LOGGER.info(f\"Freezing: {cfg.freezing}\")\n","LOGGER.info(f\"Max Length: {cfg.max_len}\")\n","LOGGER.info(f\"Num Epochs: {cfg.num_epoch}\")\n","LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.138391,"end_time":"2023-08-05T11:57:47.528988","exception":false,"start_time":"2023-08-05T11:57:47.390597","status":"completed"},"tags":[]},"outputs":[],"source":["train_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n","test_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n","submission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n","train_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n","test_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n","\n","\"\"\"train_prompts = pd.read_csv('./Data/prompts_train.csv')\n","test_prompts = pd.read_csv('./Data/prompts_test.csv')\n","submission = pd.read_csv('./Data/sample_submission.csv')\n","train_data = pd.read_csv('./Data/summaries_train.csv')\n","test_data = pd.read_csv('./Data/summaries_test.csv')\"\"\"\n","\n","print(f\"Prompt Train.shape: {train_prompts.shape}\")\n","display(train_prompts.head())\n","print(f\"Summary Train.shape: {train_data.shape}\")\n","display(train_data.head())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"fold = StratifiedKFold(n_splits=cfg.fold, shuffle=True, random_state=cfg.seed)\n","for n, (train_index, val_index) in enumerate(fold.split(train, train['prompt_id'])):\n","    train.loc[val_index, 'fold'] = n\n","train['fold'] = train['fold'].astype(int)\n","fold_sizes = train.groupby('fold').size()\n","print(fold_sizes)\"\"\"\n","\n","gkf = GroupKFold(n_splits = cfg.fold)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train_data, groups = train_data['prompt_id'])):\n","    train_data.loc[val_index, 'fold'] = i\n","    \n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, (train_index, val_index) in enumerate(gkf.split(train_data, groups = train_data['prompt_id'])):\n","    print(i)\n","    print(train_index)\n","    print(val_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.308297,"end_time":"2023-08-05T11:57:48.051785","exception":false,"start_time":"2023-08-05T11:57:47.743488","status":"completed"},"tags":[]},"outputs":[],"source":["max_words_text = train_data[\"text\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_question = train_prompts[\"prompt_question\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_text = train_prompts[\"prompt_text\"].apply(lambda x: len(x.split())).max()\n","\n","## max words\n","max_words_text, max_words_prompt_question, max_words_prompt_text"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.482642,"end_time":"2023-08-05T11:57:49.545675","exception":false,"start_time":"2023-08-05T11:57:48.063033","status":"completed"},"tags":[]},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","cfg.tokenizer = tokenizer\n","cfg.tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.021197,"end_time":"2023-08-05T11:57:49.578152","exception":false,"start_time":"2023-08-05T11:57:49.556955","status":"completed"},"tags":[]},"outputs":[],"source":["train_df = train_data.merge(train_prompts, on='prompt_id')\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.024017,"end_time":"2023-08-05T11:57:49.680128","exception":false,"start_time":"2023-08-05T11:57:49.656111","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = cfg.tokenizer\n","        self.max_len = cfg.max_len\n","        self.pq = df['prompt_question'].values\n","        self.pt = df['prompt_title'].values\n","        self.text = df['text'].values\n","        self.targets = df[['content' , 'wording']].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self , index):\n","        pq   =   self.pq[index]\n","        text =   self.text[index]\n","        pt = self.pt[index]\n","        # full_text = pq +\" \" + self.tokenizer.sep_token +\" \"+text\n","        # full_text = pt +\" \" + self.tokenizer.sep_token +\" \"+ pq + \" \" + self.tokenizer.sep_token + \" \" +text\n","        full_text = text\n","        \n","        inputs = self.tokenizer.encode_plus(\n","                        full_text,\n","                        truncation=True,\n","                        add_special_tokens=True,\n","                        max_length=self.max_len,\n","                        padding='max_length'\n","                        \n","                    )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        target = self.targets[index]\n","        \n","   \n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            \n","        } , torch.tensor(target, dtype=torch.float)\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:, :mask_len]\n","    return inputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020298,"end_time":"2023-08-05T11:57:49.712299","exception":false,"start_time":"2023-08-05T11:57:49.692001","status":"completed"},"tags":[]},"outputs":[],"source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020252,"end_time":"2023-08-05T11:57:49.744225","exception":false,"start_time":"2023-08-05T11:57:49.723973","status":"completed"},"tags":[]},"outputs":[],"source":["class MaxPooling(nn.Module):\n","    def __init__(self):\n","        super(MaxPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        embeddings = last_hidden_state.clone()\n","        embeddings[input_mask_expanded == 0] = -1e9\n","        max_embeddings, _ = torch.max(embeddings, dim = 1)\n","        return max_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020191,"end_time":"2023-08-05T11:57:49.775795","exception":false,"start_time":"2023-08-05T11:57:49.755604","status":"completed"},"tags":[]},"outputs":[],"source":["class MeanMax(nn.Module):\n","    def __init__(self):\n","        super(MeanMax, self).__init__()\n","        \n","        self.mean_pooler = MeanPooling()\n","        self.max_pooler  = MaxPooling()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        mean_pooler = self.mean_pooler( last_hidden_state ,attention_mask )\n","        max_pooler =  self.max_pooler( last_hidden_state ,attention_mask )\n","        out = torch.concat([mean_pooler ,max_pooler ] , 1)\n","        return out\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.02154,"end_time":"2023-08-05T11:57:49.808769","exception":false,"start_time":"2023-08-05T11:57:49.787229","status":"completed"},"tags":[]},"outputs":[],"source":["class GeMText(nn.Module):\n","    def __init__(self, dim = 1, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n","        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.046674,"end_time":"2023-08-05T11:57:49.867079","exception":false,"start_time":"2023-08-05T11:57:49.820405","status":"completed"},"tags":[]},"outputs":[],"source":["def get_pooling_layer():\n","    if cfg.pooling == 'Mean':\n","        return MeanPooling()\n","    \n","    elif cfg.pooling == 'Max':\n","        return MaxPooling()\n","    \n","    elif cfg.pooling == 'MeanMax':\n","        return MeanMax()\n","    \n","    elif cfg.pooling == 'GemText':\n","        return GeMText()\n","\n","\n","print(get_pooling_layer())"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.019737,"end_time":"2023-08-05T11:57:49.898637","exception":false,"start_time":"2023-08-05T11:57:49.878900","status":"completed"},"tags":[]},"outputs":[],"source":["def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","    \n","    for parameter in module.parameters():\n","        parameter.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.022006,"end_time":"2023-08-05T11:57:49.932506","exception":false,"start_time":"2023-08-05T11:57:49.910500","status":"completed"},"tags":[]},"outputs":[],"source":["def odd_layer_freeze(module):\n","    for i in range(1,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def even_layer_freeze(module):\n","    for i in range(0,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def top_half_layer_freeze(module):\n","    for i in range(0,13,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","\n","def bottom_half_layer_freeze(module):\n","    for i in range(13,14,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020541,"end_time":"2023-08-05T11:57:49.964752","exception":false,"start_time":"2023-08-05T11:57:49.944211","status":"completed"},"tags":[]},"outputs":[],"source":["\n","'''\n","## Check layers which one are freeze \n","for n,p in model.named_parameters():\n","    print(n,p.requires_grad)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.018841,"end_time":"2023-08-05T11:57:49.995581","exception":false,"start_time":"2023-08-05T11:57:49.976740","status":"completed"},"tags":[]},"outputs":[],"source":["\n","#if cfg.freezing:\n","#    top_half_layer_freeze(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.026171,"end_time":"2023-08-05T11:57:50.033939","exception":false,"start_time":"2023-08-05T11:57:50.007768","status":"completed"},"tags":[]},"outputs":[],"source":["class BaselineModel(nn.Module):\n","    def __init__(self, model_name ):\n","        super(BaselineModel, self).__init__()\n","        \n","        self.model = AutoModel.from_pretrained(cfg.model_name)\n","        self.config = AutoConfig.from_pretrained(cfg.model_name)\n","        #self.drop = nn.Dropout(p=0.2)\n","        self.pooler = get_pooling_layer()\n","\n","        if cfg.pooling == 'MeanMax':\n","            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n","        else:\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","            \n","        \n","        self._init_weights(self.fc)\n","        \n","        if cfg.freezing:\n","            top_half_layer_freeze(self.model)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","           \n","    def forward(self, ids, mask):\n","        out = self.model(input_ids=ids,attention_mask=mask,\n","                         output_hidden_states=False)\n","        out = self.pooler(out.last_hidden_state, mask)\n","        #out = self.drop(out)\n","        outputs = self.fc(out)\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.024277,"end_time":"2023-08-05T11:57:50.070421","exception":false,"start_time":"2023-08-05T11:57:50.046144","status":"completed"},"tags":[]},"outputs":[],"source":["def train_run(model ,criterion ,optimizer , dataloader):\n","    \n","    model.train()\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    running_loss = 0.0\n","    dataset_size = 0.0 \n","    \n","    \n","    for batch_idx , (data , labels) in bar:\n","        inputs , target = collate(data) , labels    \n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","        outputs = model(ids, mask)\n","        loss = criterion(outputs, targets)\n","        \n","        # normalize loss to account for batch accumulation\n","        loss = loss / cfg.accum_iter \n","        loss.backward()\n","        \n","        if ((batch_idx + 1) % cfg.accum_iter == 0) or (batch_idx + 1 == len(dataloader)):\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","\n","    epoch_loss = running_loss/dataset_size\n","    gc.collect()\n","    \n","\n","    \n","    return epoch_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.024256,"end_time":"2023-08-05T11:57:50.107068","exception":false,"start_time":"2023-08-05T11:57:50.082812","status":"completed"},"tags":[]},"outputs":[],"source":["@torch.no_grad()\n","def valid_run(model , dataloader):\n","    model.eval()\n","    \n","    running_loss = 0.0\n","    dataset_size = 0.0\n","    \n","    predictions = []\n","    y_labels = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for batch_idx , (data , labels) in bar:\n","        inputs , target = collate(data) , labels\n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","\n","        outputs = model(ids, mask)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        predictions.append(outputs.detach().to('cpu').numpy())\n","        y_labels.append(labels.detach().to('cpu').numpy())\n","    \n","    predictions = np.concatenate(predictions)\n","    y_labels    = np.concatenate(y_labels)\n","    epoch_loss = running_loss / dataset_size\n","    gc.collect()   \n","    \n","    return epoch_loss , predictions , y_labels\n","        \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020672,"end_time":"2023-08-05T11:57:50.139824","exception":false,"start_time":"2023-08-05T11:57:50.119152","status":"completed"},"tags":[]},"outputs":[],"source":["def prepare_fold(fold):\n","    \n","    dftrain = train_df[train_df['fold']!= fold]\n","    dfvalid = train_df[train_df['fold']== fold]\n","    \n","    train_dataset = TrainDataset(dftrain)\n","    valid_dataset = TrainDataset(dfvalid)\n","    \n","    train_loader = DataLoader(train_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n","    valid_loader = DataLoader(valid_dataset ,batch_size=cfg.batch_size,num_workers=2, shuffle=True, pin_memory=True)\n","    \n","    return train_loader , valid_loader\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader , valid_loader = prepare_fold(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.020078,"end_time":"2023-08-05T11:57:50.172036","exception":false,"start_time":"2023-08-05T11:57:50.151958","status":"completed"},"tags":[]},"outputs":[],"source":["def oof_df(n_fold , true , pred):\n","    \n","    df_pred = pd.DataFrame(pred ,columns= ['pred_content' , 'pred_wording'] )\n","    df_real = pd.DataFrame(true ,columns= ['content' , 'wording'] )\n","    \n","    df = pd.concat([df_real , df_pred ],1)\n","\n","    \n","    return df\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.022134,"end_time":"2023-08-05T11:57:50.206375","exception":false,"start_time":"2023-08-05T11:57:50.184241","status":"completed"},"tags":[]},"outputs":[],"source":["def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","              'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":10635.997725,"end_time":"2023-08-05T14:55:06.216462","exception":false,"start_time":"2023-08-05T11:57:50.218737","status":"completed"},"tags":[]},"outputs":[],"source":["oof_dfs = []\n","for n_fold in range(cfg.fold):\n","    LOGGER.info('\\n')\n","    LOGGER.info(f\"========== fold: {n_fold} training ==========\")\n","    train_loader, valid_loader = prepare_fold(fold=n_fold)\n","    LOGGER.info(f'Number of batches in Train {len(train_loader) } and valid {len(valid_loader)} dataset')\n","    model  = BaselineModel(cfg.model_name).to(cfg.device)   \n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=cfg.encoder_lr, \n","                                                decoder_lr=cfg.decoder_lr,\n","                                                weight_decay=cfg.weight_decay)\n","\n","    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n","    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.T_max, \n","                                                   eta_min=cfg.min_lr)    \n","    \n","   \n","    criterion = nn.SmoothL1Loss(reduction='mean')\n","    \n","    start = time.time()\n","    best_epoch_score = np.inf\n","    for epoch in range(cfg.num_epoch):\n","        \n","        train_loss  = train_run(model ,criterion ,optimizer , dataloader=train_loader)\n","        valid_loss , valid_preds , valid_labels  = valid_run(model , dataloader=valid_loader)\n","        \n","        if valid_loss < best_epoch_score:\n","            \n","            LOGGER.info(f\"Validation Loss Improved ({best_epoch_score} ---> {valid_loss})\")\n","            best_epoch_score = valid_loss\n","            ### saving weights\n","            torch.save(model.state_dict(), f\"{cfg.only_model_name}_Fold_{n_fold}.pth\") \n","            \n","            ## saving oof values\n","            df_ = oof_df(n_fold , valid_labels , valid_preds)\n","            \n","            LOGGER.info(f'Weights and oof values saved for epochs-{epoch} .....')\n","            \n","        LOGGER.info(f\"Epoch {epoch} Training Loss {np.round(train_loss , 4)} Validation Loss {np.round(valid_loss , 4)}\")\n","    \n","        \n","    end = time.time()\n","    time_elapsed = end - start\n","    \n","    LOGGER.info(' Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    \n","    LOGGER.info(\" Best Loss: {:.4f}\".format(best_epoch_score))\n","    \n","    oof_dfs.append(df_)            \n","    LOGGER.info(f\" oof for fold {n_fold} ---> {score_loss(valid_labels, valid_preds )}\")\n","    del model, train_loader, valid_loader , df_ , valid_preds , valid_labels\n","    gc.collect()\n","    LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.084269,"end_time":"2023-08-05T14:55:06.325902","exception":false,"start_time":"2023-08-05T14:55:06.241633","status":"completed"},"tags":[]},"outputs":[],"source":["oof_df = pd.concat(oof_dfs , ignore_index=True )\n","oof_df.to_csv('oof_df.csv' , index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_trues_final = np.array([oof_df[\"content\"].to_list(), oof_df['wording'].to_list()]).T\n","y_preds_final  = np.array([oof_df[\"pred_content\"].to_list(), oof_df[\"pred_wording\"].to_list()]).T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(MCRMSE(y_trues_final, y_preds_final))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
