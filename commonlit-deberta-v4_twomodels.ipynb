{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"papermill":{"duration":18.376581,"end_time":"2023-08-05T11:57:47.268900","exception":false,"start_time":"2023-08-05T11:57:28.892319","status":"completed"},"tags":[]},"outputs":[],"source":["\n","import os\n","import gc\n","from tqdm.auto import tqdm\n","import transformers\n","import numpy as np \n","import pandas as pd \n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error\n","import plotly.express as px #graphing\n","import plotly.graph_objects as go #graphing\n","from plotly.subplots import make_subplots #graphing\n","import plotly.figure_factory as ff #graphing\n","from torch.nn.parameter import Parameter\n","from transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n","from transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n","\n","\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","from datasets import concatenate_datasets,load_dataset,load_from_disk\n","\n","from sklearn.metrics import log_loss\n","\n","from transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","import time\n","import warnings\n","import collections\n","# from termcolor import colored\n","\n","from torch.optim import lr_scheduler\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"papermill":{"duration":0.016027,"end_time":"2023-08-05T11:57:47.293377","exception":false,"start_time":"2023-08-05T11:57:47.277350","status":"completed"},"tags":[]},"outputs":[],"source":["OUTPUT_DIR = './'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","    "]},{"cell_type":"code","execution_count":3,"metadata":{"papermill":{"duration":0.015833,"end_time":"2023-08-05T11:57:47.317500","exception":false,"start_time":"2023-08-05T11:57:47.301667","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import (\n","    AutoModel, AutoConfig, \n","    AutoTokenizer, logging\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"papermill":{"duration":0.055438,"end_time":"2023-08-05T11:57:47.381677","exception":false,"start_time":"2023-08-05T11:57:47.326239","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["=========================== Model name :deberta-v3-base ===========================: \n","\n","\n","Scheduler: CosineAnnealingLR\n","batch_size: 2 with gradient Accumukation 16 \n","Pooling name: GemText \n","Freezing: False\n","Max Length: 512\n","Num Epochs: 1\n","\n","\n"]}],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","\n","\n","class cfg:\n","    select = 'base'\n","    # model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n","    model_name = f'./Models/deberta-v3-{select}'\n","\n","    only_model_name = f'deberta-v3-{select}'\n","    accum_iter = 16\n","    fold = 4\n","    split = 5\n","    seed = 42\n","    batch_size = 2\n","    max_len = 512\n","    num_epoch = 1\n","    T_max= 500\n","    \n","    scheduler = 'CosineAnnealingLR'\n","    weight_decay =  1e-6\n","    min_lr = 1e-6\n","    freezing = False\n","    pooling = 'GemText'\n","    weight_decay = 1e-2\n","    encoder_lr = 1e-5\n","    decoder_lr = 1e-5\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","\n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n","        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","\n","def score_loss(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return {\n","        'mcrmse_score' : mcrmse_score,\n","        'Content_score' : scores[0],\n","        'Wording_score' : scores[1]\n","    }\n","\n","def get_logger(filename='Training'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","\n","def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","set_seed(cfg.seed)\n","\n","\n","LOGGER.info(f\"=========================== Model name :{cfg.only_model_name} ===========================: \")\n","LOGGER.info('\\n')\n","LOGGER.info(f\"Scheduler: {cfg.scheduler}\")\n","LOGGER.info(f\"batch_size: {cfg.batch_size} with gradient Accumukation {cfg.accum_iter} \")\n","LOGGER.info(f\"Pooling name: {cfg.pooling} \")\n","LOGGER.info(f\"Freezing: {cfg.freezing}\")\n","LOGGER.info(f\"Max Length: {cfg.max_len}\")\n","LOGGER.info(f\"Num Epochs: {cfg.num_epoch}\")\n","LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":5,"metadata":{"papermill":{"duration":0.138391,"end_time":"2023-08-05T11:57:47.528988","exception":false,"start_time":"2023-08-05T11:57:47.390597","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt Train.shape: (4, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt_id</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39c16e</td>\n","      <td>Summarize at least 3 elements of an ideal trag...</td>\n","      <td>On Tragedy</td>\n","      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3b9047</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>814d6b</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ebad26</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  prompt_id                                    prompt_question  \\\n","0    39c16e  Summarize at least 3 elements of an ideal trag...   \n","1    3b9047  In complete sentences, summarize the structure...   \n","2    814d6b  Summarize how the Third Wave developed over su...   \n","3    ebad26  Summarize the various ways the factory would u...   \n","\n","                prompt_title  \\\n","0                 On Tragedy   \n","1  Egyptian Social Structure   \n","2             The Third Wave   \n","3    Excerpt from The Jungle   \n","\n","                                         prompt_text  \n","0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n","1  Egyptian society was structured like a pyramid...  \n","2  Background \\r\\nThe Third Wave experiment took ...  \n","3  With one member trimming beef in a cannery, an...  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Summary Train.shape: (7165, 5)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  \n","0  0.205683  0.380538  \n","1 -0.548304  0.506755  \n","2  3.128928  4.231226  \n","3 -0.210614 -0.471415  \n","4  3.272894  3.219757  "]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"train_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n","test_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n","submission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n","train_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n","test_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\"\"\"\n","\n","train_prompts = pd.read_csv('./Data/prompts_train.csv')\n","test_prompts = pd.read_csv('./Data/prompts_test.csv')\n","submission = pd.read_csv('./Data/sample_submission.csv')\n","train_data = pd.read_csv('./Data/summaries_train.csv')\n","test_data = pd.read_csv('./Data/summaries_test.csv')\n","\n","print(f\"Prompt Train.shape: {train_prompts.shape}\")\n","display(train_prompts.head())\n","print(f\"Summary Train.shape: {train_data.shape}\")\n","display(train_data.head())\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  fold  \n","0  0.205683  0.380538   3.0  \n","1 -0.548304  0.506755   2.0  \n","2  3.128928  4.231226   1.0  \n","3 -0.210614 -0.471415   1.0  \n","4  3.272894  3.219757   3.0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"fold = StratifiedKFold(n_splits=cfg.fold, shuffle=True, random_state=cfg.seed)\n","for n, (train_index, val_index) in enumerate(fold.split(train, train['prompt_id'])):\n","    train.loc[val_index, 'fold'] = n\n","train['fold'] = train['fold'].astype(int)\n","fold_sizes = train.groupby('fold').size()\n","print(fold_sizes)\"\"\"\n","\n","gkf = GroupKFold(n_splits = cfg.fold)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train_data, groups = train_data['prompt_id'])):\n","    train_data.loc[val_index, 'fold'] = i\n","    \n","train_data.head()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","[   0    1    2 ... 7161 7162 7164]\n","[   8    9   13 ... 7146 7157 7163]\n","1\n","[   0    1    4 ... 7160 7163 7164]\n","[   2    3    6 ... 7159 7161 7162]\n","2\n","[   0    2    3 ... 7161 7162 7163]\n","[   1    5    7 ... 7155 7160 7164]\n","3\n","[   1    2    3 ... 7162 7163 7164]\n","[   0    4   14 ... 7141 7144 7156]\n"]}],"source":["for i, (train_index, val_index) in enumerate(gkf.split(train_data, groups = train_data['prompt_id'])):\n","    print(i)\n","    print(train_index)\n","    print(val_index)"]},{"cell_type":"code","execution_count":8,"metadata":{"papermill":{"duration":0.308297,"end_time":"2023-08-05T11:57:48.051785","exception":false,"start_time":"2023-08-05T11:57:47.743488","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(647, 27, 966)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["max_words_text = train_data[\"text\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_question = train_prompts[\"prompt_question\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_text = train_prompts[\"prompt_text\"].apply(lambda x: len(x.split())).max()\n","\n","## max words\n","max_words_text, max_words_prompt_question, max_words_prompt_text"]},{"cell_type":"code","execution_count":9,"metadata":{"papermill":{"duration":1.482642,"end_time":"2023-08-05T11:57:49.545675","exception":false,"start_time":"2023-08-05T11:57:48.063033","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["DebertaV2TokenizerFast(name_or_path='./Models/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","cfg.tokenizer = tokenizer\n","cfg.tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"papermill":{"duration":0.021197,"end_time":"2023-08-05T11:57:49.578152","exception":false,"start_time":"2023-08-05T11:57:49.556955","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>fold</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>3.0</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>3.0</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0095993991fe</td>\n","      <td>814d6b</td>\n","      <td>The third wave only started as an experiment w...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>3.0</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00c20c6ddd23</td>\n","      <td>814d6b</td>\n","      <td>The experimen was orginally about how even whe...</td>\n","      <td>0.567975</td>\n","      <td>0.969062</td>\n","      <td>3.0</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00d40ad10dc9</td>\n","      <td>814d6b</td>\n","      <td>The third wave developed so quickly due to the...</td>\n","      <td>-0.910596</td>\n","      <td>-0.081769</td>\n","      <td>3.0</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","2  0095993991fe    814d6b  The third wave only started as an experiment w...   \n","3  00c20c6ddd23    814d6b  The experimen was orginally about how even whe...   \n","4  00d40ad10dc9    814d6b  The third wave developed so quickly due to the...   \n","\n","    content   wording  fold  \\\n","0  0.205683  0.380538   3.0   \n","1  3.272894  3.219757   3.0   \n","2  0.205683  0.380538   3.0   \n","3  0.567975  0.969062   3.0   \n","4 -0.910596 -0.081769   3.0   \n","\n","                                     prompt_question    prompt_title  \\\n","0  Summarize how the Third Wave developed over su...  The Third Wave   \n","1  Summarize how the Third Wave developed over su...  The Third Wave   \n","2  Summarize how the Third Wave developed over su...  The Third Wave   \n","3  Summarize how the Third Wave developed over su...  The Third Wave   \n","4  Summarize how the Third Wave developed over su...  The Third Wave   \n","\n","                                         prompt_text  \n","0  Background \\r\\nThe Third Wave experiment took ...  \n","1  Background \\r\\nThe Third Wave experiment took ...  \n","2  Background \\r\\nThe Third Wave experiment took ...  \n","3  Background \\r\\nThe Third Wave experiment took ...  \n","4  Background \\r\\nThe Third Wave experiment took ...  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_df = train_data.merge(train_prompts, on='prompt_id')\n","train_df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"papermill":{"duration":0.024017,"end_time":"2023-08-05T11:57:49.680128","exception":false,"start_time":"2023-08-05T11:57:49.656111","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainContent(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = cfg.tokenizer\n","        self.max_len = cfg.max_len\n","        self.pq = df['prompt_question'].values\n","        self.pt = df['prompt_title'].values\n","        self.text = df['text'].values\n","        self.targets = df['content'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self , index):\n","        pq   =   self.pq[index]\n","        text =   self.text[index]\n","        pt = self.pt[index]\n","        # full_text = pq +\" \" + self.tokenizer.sep_token +\" \"+text\n","        # full_text = pt +\" \" + self.tokenizer.sep_token +\" \"+ pq + \" \" + self.tokenizer.sep_token + \" \" +text\n","        full_text = text\n","        \n","        inputs = self.tokenizer.encode_plus(\n","                        full_text,\n","                        truncation=True,\n","                        add_special_tokens=True,\n","                        max_length=self.max_len,\n","                        padding='max_length'\n","                        \n","                    )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        target = self.target[index]\n","        \n","   \n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            \n","        } , torch.tensor(target, dtype=torch.float)\n","    \n","class TrainWord(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = cfg.tokenizer\n","        self.max_len = cfg.max_len\n","        self.pq = df['prompt_question'].values\n","        self.pt = df['prompt_title'].values\n","        self.text = df['text'].values\n","        self.target = df['wording'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self , index):\n","        pq   =   self.pq[index]\n","        text =   self.text[index]\n","        pt = self.pt[index]\n","        # full_text = pq +\" \" + self.tokenizer.sep_token +\" \"+text\n","        # full_text = pt +\" \" + self.tokenizer.sep_token +\" \"+ pq + \" \" + self.tokenizer.sep_token + \" \" +text\n","        full_text = text\n","        \n","        inputs = self.tokenizer.encode_plus(\n","                        full_text,\n","                        truncation=True,\n","                        add_special_tokens=True,\n","                        max_length=self.max_len,\n","                        padding='max_length'\n","                        \n","                    )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        target = self.target[index]\n","        \n","   \n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            \n","        } , torch.tensor(target, dtype=torch.float)\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:, :mask_len]\n","    return inputs\n"]},{"cell_type":"code","execution_count":12,"metadata":{"papermill":{"duration":0.020298,"end_time":"2023-08-05T11:57:49.712299","exception":false,"start_time":"2023-08-05T11:57:49.692001","status":"completed"},"tags":[]},"outputs":[],"source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings"]},{"cell_type":"code","execution_count":13,"metadata":{"papermill":{"duration":0.020252,"end_time":"2023-08-05T11:57:49.744225","exception":false,"start_time":"2023-08-05T11:57:49.723973","status":"completed"},"tags":[]},"outputs":[],"source":["class MaxPooling(nn.Module):\n","    def __init__(self):\n","        super(MaxPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        embeddings = last_hidden_state.clone()\n","        embeddings[input_mask_expanded == 0] = -1e9\n","        max_embeddings, _ = torch.max(embeddings, dim = 1)\n","        return max_embeddings"]},{"cell_type":"code","execution_count":14,"metadata":{"papermill":{"duration":0.020191,"end_time":"2023-08-05T11:57:49.775795","exception":false,"start_time":"2023-08-05T11:57:49.755604","status":"completed"},"tags":[]},"outputs":[],"source":["class MeanMax(nn.Module):\n","    def __init__(self):\n","        super(MeanMax, self).__init__()\n","        \n","        self.mean_pooler = MeanPooling()\n","        self.max_pooler  = MaxPooling()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        mean_pooler = self.mean_pooler( last_hidden_state ,attention_mask )\n","        max_pooler =  self.max_pooler( last_hidden_state ,attention_mask )\n","        out = torch.concat([mean_pooler ,max_pooler ] , 1)\n","        return out\n","    "]},{"cell_type":"code","execution_count":15,"metadata":{"papermill":{"duration":0.02154,"end_time":"2023-08-05T11:57:49.808769","exception":false,"start_time":"2023-08-05T11:57:49.787229","status":"completed"},"tags":[]},"outputs":[],"source":["class GeMText(nn.Module):\n","    def __init__(self, dim = 1, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n","        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret"]},{"cell_type":"code","execution_count":16,"metadata":{"papermill":{"duration":0.046674,"end_time":"2023-08-05T11:57:49.867079","exception":false,"start_time":"2023-08-05T11:57:49.820405","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["GeMText()\n"]}],"source":["def get_pooling_layer():\n","    if cfg.pooling == 'Mean':\n","        return MeanPooling()\n","    \n","    elif cfg.pooling == 'Max':\n","        return MaxPooling()\n","    \n","    elif cfg.pooling == 'MeanMax':\n","        return MeanMax()\n","    \n","    elif cfg.pooling == 'GemText':\n","        return GeMText()\n","\n","\n","print(get_pooling_layer())"]},{"cell_type":"code","execution_count":17,"metadata":{"papermill":{"duration":0.019737,"end_time":"2023-08-05T11:57:49.898637","exception":false,"start_time":"2023-08-05T11:57:49.878900","status":"completed"},"tags":[]},"outputs":[],"source":["def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","    \n","    for parameter in module.parameters():\n","        parameter.requires_grad = False"]},{"cell_type":"code","execution_count":18,"metadata":{"papermill":{"duration":0.022006,"end_time":"2023-08-05T11:57:49.932506","exception":false,"start_time":"2023-08-05T11:57:49.910500","status":"completed"},"tags":[]},"outputs":[],"source":["def odd_layer_freeze(module):\n","    for i in range(1,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def even_layer_freeze(module):\n","    for i in range(0,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def top_half_layer_freeze(module):\n","    for i in range(0,13,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","\n","def bottom_half_layer_freeze(module):\n","    for i in range(13,14,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"papermill":{"duration":0.020541,"end_time":"2023-08-05T11:57:49.964752","exception":false,"start_time":"2023-08-05T11:57:49.944211","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'\\n## Check layers which one are freeze \\nfor n,p in model.named_parameters():\\n    print(n,p.requires_grad)\\n'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\n","'''\n","## Check layers which one are freeze \n","for n,p in model.named_parameters():\n","    print(n,p.requires_grad)\n","'''"]},{"cell_type":"code","execution_count":20,"metadata":{"papermill":{"duration":0.018841,"end_time":"2023-08-05T11:57:49.995581","exception":false,"start_time":"2023-08-05T11:57:49.976740","status":"completed"},"tags":[]},"outputs":[],"source":["\n","#if cfg.freezing:\n","#    top_half_layer_freeze(model)"]},{"cell_type":"code","execution_count":21,"metadata":{"papermill":{"duration":0.026171,"end_time":"2023-08-05T11:57:50.033939","exception":false,"start_time":"2023-08-05T11:57:50.007768","status":"completed"},"tags":[]},"outputs":[],"source":["class BaselineModel(nn.Module):\n","    def __init__(self, model_name ):\n","        super(BaselineModel, self).__init__()\n","        \n","        self.model = AutoModel.from_pretrained(cfg.model_name)\n","        self.config = AutoConfig.from_pretrained(cfg.model_name)\n","        #self.drop = nn.Dropout(p=0.2)\n","        self.pooler = get_pooling_layer()\n","\n","        if cfg.pooling == 'MeanMax':\n","            self.fc = nn.Linear(2*self.config.hidden_size, 1)\n","        else:\n","            self.fc = nn.Linear(self.config.hidden_size, 1)\n","            \n","        \n","        self._init_weights(self.fc)\n","        \n","        if cfg.freezing:\n","            top_half_layer_freeze(self.model)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","           \n","    def forward(self, ids, mask):\n","        out = self.model(input_ids=ids,attention_mask=mask,\n","                         output_hidden_states=False)\n","        out = self.pooler(out.last_hidden_state, mask)\n","        #out = self.drop(out)\n","        outputs = self.fc(out)\n","        return outputs"]},{"cell_type":"code","execution_count":22,"metadata":{"papermill":{"duration":0.024277,"end_time":"2023-08-05T11:57:50.070421","exception":false,"start_time":"2023-08-05T11:57:50.046144","status":"completed"},"tags":[]},"outputs":[],"source":["def train_run(model ,criterion ,optimizer , dataloader):\n","    \n","    model.train()\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    running_loss = 0.0\n","    dataset_size = 0.0 \n","    \n","    \n","    for batch_idx , (data , labels) in bar:\n","        inputs , target = collate(data) , labels    \n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","        outputs = model(ids, mask)\n","        loss = criterion(outputs, targets)\n","        \n","        # normalize loss to account for batch accumulation\n","        loss = loss / cfg.accum_iter \n","        loss.backward()\n","        \n","        if ((batch_idx + 1) % cfg.accum_iter == 0) or (batch_idx + 1 == len(dataloader)):\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","\n","    epoch_loss = running_loss/dataset_size\n","    gc.collect()\n","    \n","\n","    \n","    return epoch_loss\n"]},{"cell_type":"code","execution_count":23,"metadata":{"papermill":{"duration":0.024256,"end_time":"2023-08-05T11:57:50.107068","exception":false,"start_time":"2023-08-05T11:57:50.082812","status":"completed"},"tags":[]},"outputs":[],"source":["@torch.no_grad()\n","def valid_run(model , dataloader):\n","    model.eval()\n","    \n","    running_loss = 0.0\n","    dataset_size = 0.0\n","    \n","    predictions = []\n","    y_labels = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for batch_idx , (data , labels) in bar:\n","        inputs , target = collate(data) , labels\n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","\n","        outputs = model(ids, mask)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        predictions.append(outputs.detach().to('cpu').numpy())\n","        y_labels.append(labels.detach().to('cpu').numpy())\n","    \n","    predictions = np.concatenate(predictions)\n","    y_labels    = np.concatenate(y_labels)\n","    epoch_loss = running_loss / dataset_size\n","    gc.collect()   \n","    \n","    return epoch_loss , predictions , y_labels\n","        \n","    "]},{"cell_type":"code","execution_count":24,"metadata":{"papermill":{"duration":0.020672,"end_time":"2023-08-05T11:57:50.139824","exception":false,"start_time":"2023-08-05T11:57:50.119152","status":"completed"},"tags":[]},"outputs":[],"source":["def prepare_fold(fold):\n","    \n","    dftrain = train_df[train_df['fold']!= fold]\n","    dfvalid = train_df[train_df['fold']== fold]\n","    \n","    train_content_dataset = TrainContent(dftrain)\n","    valid_content_dataset = TrainContent(dfvalid)\n","    \n","    train_content_loader = DataLoader(train_content_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n","    valid_content_loader = DataLoader(valid_content_dataset ,batch_size=cfg.batch_size,num_workers=2, shuffle=True, pin_memory=True)\n","    \n","    train_word_dataset = TrainWord(dftrain)\n","    valid_word_dataset = TrainWord(dfvalid)\n","    \n","    train_word_loader = DataLoader(train_word_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n","    valid_word_loader = DataLoader(valid_word_dataset ,batch_size=cfg.batch_size,num_workers=2, shuffle=True, pin_memory=True)\n","    \n","    return train_word_loader , valid_word_loader, train_content_loader , valid_content_loader\n","    "]},{"cell_type":"code","execution_count":25,"metadata":{"papermill":{"duration":0.020078,"end_time":"2023-08-05T11:57:50.172036","exception":false,"start_time":"2023-08-05T11:57:50.151958","status":"completed"},"tags":[]},"outputs":[],"source":["def oof_df(option , true , pred):\n","    \n","    if option == 'word':\n","        df_pred = pd.DataFrame(pred ,columns= ['pred_wording'] )\n","        df_real = pd.DataFrame(true ,columns= ['wording'] )\n","        \n","        df = pd.concat([df_real , df_pred ],1)\n","    \n","    if option == 'content':\n","        df_pred = pd.DataFrame(pred ,columns= ['pred_content'] )\n","        df_real = pd.DataFrame(true ,columns= ['content'] )\n","        \n","        df = pd.concat([df_real , df_pred ],1)\n","\n","    \n","    return df"]},{"cell_type":"code","execution_count":26,"metadata":{"papermill":{"duration":0.022134,"end_time":"2023-08-05T11:57:50.206375","exception":false,"start_time":"2023-08-05T11:57:50.184241","status":"completed"},"tags":[]},"outputs":[],"source":["def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","              'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters"]},{"cell_type":"code","execution_count":27,"metadata":{"papermill":{"duration":10635.997725,"end_time":"2023-08-05T14:55:06.216462","exception":false,"start_time":"2023-08-05T11:57:50.218737","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","\n","========== fold: 0 training ==========\n","  1%|‚ñè         | 38/2554 [00:02<02:44, 15.34it/s]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 9.77 GiB total capacity; 5.15 GiB already allocated; 54.06 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[1;32m/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m best_epoch_score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg\u001b[39m.\u001b[39mnum_epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     train_loss  \u001b[39m=\u001b[39m train_run(modelword ,criterion ,optimizer_word , dataloader\u001b[39m=\u001b[39mtrain_word_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     valid_loss , valid_preds , valid_labels  \u001b[39m=\u001b[39m valid_run(modelword , dataloader\u001b[39m=\u001b[39mvalid_word_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_epoch_score:\n","\u001b[1;32m/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m targets \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(cfg\u001b[39m.\u001b[39mdevice, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batch_size \u001b[39m=\u001b[39m ids\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(ids, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# normalize loss to account for batch accumulation\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32m/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, ids, mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(input_ids\u001b[39m=\u001b[39mids,attention_mask\u001b[39m=\u001b[39mmask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                      output_hidden_states\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(out\u001b[39m.\u001b[39mlast_hidden_state, mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/frank/Applications/PythonProjs/CommonLit/commonlit-deberta-v4_twomodels.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m#out = self.drop(out)\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1075\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1076\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1077\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1081\u001b[0m )\n\u001b[0;32m-> 1083\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1084\u001b[0m     embedding_output,\n\u001b[1;32m   1085\u001b[0m     attention_mask,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1089\u001b[0m )\n\u001b[1;32m   1090\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         rel_embeddings,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    521\u001b[0m         next_kv,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         query_states\u001b[39m=\u001b[39mquery_states,\n\u001b[1;32m    524\u001b[0m         relative_pos\u001b[39m=\u001b[39mrelative_pos,\n\u001b[1;32m    525\u001b[0m         rel_embeddings\u001b[39m=\u001b[39mrel_embeddings,\n\u001b[1;32m    526\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    530\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    363\u001b[0m         hidden_states,\n\u001b[1;32m    364\u001b[0m         attention_mask,\n\u001b[1;32m    365\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    366\u001b[0m         query_states\u001b[39m=\u001b[39mquery_states,\n\u001b[1;32m    367\u001b[0m         relative_pos\u001b[39m=\u001b[39mrelative_pos,\n\u001b[1;32m    368\u001b[0m         rel_embeddings\u001b[39m=\u001b[39mrel_embeddings,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    294\u001b[0m         hidden_states,\n\u001b[1;32m    295\u001b[0m         attention_mask,\n\u001b[1;32m    296\u001b[0m         output_attentions,\n\u001b[1;32m    297\u001b[0m         query_states\u001b[39m=\u001b[39mquery_states,\n\u001b[1;32m    298\u001b[0m         relative_pos\u001b[39m=\u001b[39mrelative_pos,\n\u001b[1;32m    299\u001b[0m         rel_embeddings\u001b[39m=\u001b[39mrel_embeddings,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:727\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    726\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 727\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisentangled_attention_bias(\n\u001b[1;32m    728\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n","File \u001b[0;32m~/miniconda3/envs/TorchHuggingFace/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:803\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mc2p\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_att_type:\n\u001b[1;32m    802\u001b[0m     scale \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(pos_key_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat) \u001b[39m*\u001b[39m scale_factor)\n\u001b[0;32m--> 803\u001b[0m     c2p_att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(query_layer, pos_key_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m    804\u001b[0m     c2p_pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(relative_pos \u001b[39m+\u001b[39m att_span, \u001b[39m0\u001b[39m, att_span \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    805\u001b[0m     c2p_att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    806\u001b[0m         c2p_att,\n\u001b[1;32m    807\u001b[0m         dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    808\u001b[0m         index\u001b[39m=\u001b[39mc2p_pos\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand([query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), query_layer\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), relative_pos\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)]),\n\u001b[1;32m    809\u001b[0m     )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 9.77 GiB total capacity; 5.15 GiB already allocated; 54.06 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["oof_dfs_word = []\n","oof_dfs_content = []\n","\n","for n_fold in range(cfg.fold):\n","    LOGGER.info('\\n')\n","    LOGGER.info(f\"========== fold: {n_fold} training ==========\")\n","    train_word_loader , valid_word_loader, train_content_loader , valid_content_loader = prepare_fold(fold=n_fold)\n","\n","    modelword  = BaselineModel(cfg.model_name).to(cfg.device)   \n","    modelcontent  = BaselineModel(cfg.model_name).to(cfg.device)   \n","\n","    optimizer_parameters_word = get_optimizer_params(modelword,\n","                                                encoder_lr=cfg.encoder_lr, \n","                                                decoder_lr=cfg.decoder_lr,\n","                                                weight_decay=cfg.weight_decay)\n","    \n","    optimizer_parameters_content = get_optimizer_params(modelcontent,\n","                                                encoder_lr=cfg.encoder_lr, \n","                                                decoder_lr=cfg.decoder_lr,\n","                                                weight_decay=cfg.weight_decay)\n","\n","    optimizer_word = AdamW(optimizer_parameters_word, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n","    optimizer_content = AdamW(optimizer_parameters_content, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n","\n","    scheduler_word = lr_scheduler.CosineAnnealingLR(optimizer_word,T_max=cfg.T_max, eta_min=cfg.min_lr)    \n","    scheduler_content = lr_scheduler.CosineAnnealingLR(optimizer_content,T_max=cfg.T_max, eta_min=cfg.min_lr)\n","   \n","    criterion = nn.SmoothL1Loss(reduction='mean')\n","    \n","    start = time.time()\n","    best_epoch_score = np.inf\n","    \n","    for epoch in range(cfg.num_epoch):\n","        \n","        train_loss  = train_run(modelword ,criterion ,optimizer_word , dataloader=train_word_loader)\n","        valid_loss , valid_preds , valid_labels  = valid_run(modelword , dataloader=valid_word_loader)\n","        \n","        if valid_loss < best_epoch_score:\n","            \n","            LOGGER.info(f\"Validation Loss Improved ({best_epoch_score} ---> {valid_loss})\")\n","            best_epoch_score = valid_loss\n","            ### saving weights\n","            torch.save(modelword.state_dict(), f\"{cfg.only_model_name}_word_Fold_{n_fold}.pth\") \n","            \n","            ## saving oof values\n","            df_word = oof_df('word', valid_labels , valid_preds)\n","            \n","            LOGGER.info(f'Weights and oof values saved for epochs-{epoch} .....')\n","            \n","        LOGGER.info(f\"Epoch {epoch} Training Loss {np.round(train_loss , 4)} Validation Loss {np.round(valid_loss , 4)}\")\n","    \n","    for epoch in range(cfg.num_epoch):\n","        \n","        train_loss  = train_run(modelcontent ,criterion ,optimizer_content , dataloader=train_content_loader)\n","        valid_loss , valid_preds , valid_labels  = valid_run(modelcontent , dataloader=valid_content_loader)\n","        \n","        if valid_loss < best_epoch_score:\n","            \n","            LOGGER.info(f\"Validation Loss Improved ({best_epoch_score} ---> {valid_loss})\")\n","            best_epoch_score = valid_loss\n","            ### saving weights\n","            torch.save(modelcontent.state_dict(), f\"{cfg.only_model_name}_content_Fold_{n_fold}.pth\") \n","            \n","            ## saving oof values\n","            df_content = oof_df('content' , valid_labels , valid_preds)\n","            \n","            LOGGER.info(f'Weights and oof values saved for epochs-{epoch} .....')\n","            \n","        LOGGER.info(f\"Epoch {epoch} Training Loss {np.round(train_loss , 4)} Validation Loss {np.round(valid_loss , 4)}\")\n","        \n","    end = time.time()\n","    time_elapsed = end - start\n","    \n","    LOGGER.info(' Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    \n","    LOGGER.info(\" Best Loss: {:.4f}\".format(best_epoch_score))\n","    \n","    oof_dfs_word.append(df_word)\n","    oof_dfs_content.append(df_content)            \n","            \n","    LOGGER.info(f\" oof for fold {n_fold} ---> {score_loss(valid_labels, valid_preds )}\")\n","    del modelword, train_word_loader, valid_word_loader , df_word , valid_preds , valid_labels\n","    del modelcontent, train_content_loader, valid_content_loader , df_content\n","\n","    gc.collect()\n","    LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.084269,"end_time":"2023-08-05T14:55:06.325902","exception":false,"start_time":"2023-08-05T14:55:06.241633","status":"completed"},"tags":[]},"outputs":[],"source":["oof_df_word = pd.concat(oof_dfs_word , ignore_index=True )\n","oof_df_content = pd.concat(oof_dfs_content , ignore_index=True )\n","\n","#oof_df.to_csv('oof_df.csv' , index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_trues_final = np.array([oof_df[\"content\"].to_list(), oof_df['wording'].to_list()]).T\n","y_preds_final  = np.array([oof_df[\"pred_content\"].to_list(), oof_df[\"pred_wording\"].to_list()]).T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(MCRMSE(y_trues_final, y_preds_final))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
