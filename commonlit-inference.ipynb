{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nfrom tqdm.auto import tqdm\nimport transformers\nimport numpy as np \nimport pandas as pd \nfrom torch.nn.parameter import Parameter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport plotly.express as px #graphing\nimport plotly.graph_objects as go #graphing\nfrom plotly.subplots import make_subplots #graphing\nimport plotly.figure_factory as ff #graphing\n\nfrom transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\nfrom transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\nfrom transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n\n\nfrom text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\nfrom datasets import concatenate_datasets,load_dataset,load_from_disk\n\nfrom sklearn.metrics import log_loss\n\nfrom transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport time\nimport warnings\nimport collections\nfrom termcolor import colored\n\nfrom torch.optim import lr_scheduler\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:13:18.431882Z","iopub.execute_input":"2023-09-19T16:13:18.432365Z","iopub.status.idle":"2023-09-19T16:13:36.480029Z","shell.execute_reply.started":"2023-09-19T16:13:18.432324Z","shell.execute_reply":"2023-09-19T16:13:36.478776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef score_loss(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return {\n        'mcrmse_score' : mcrmse_score,\n        'Content_score' : scores[0],\n        'Wording_score' : scores[1]\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:13:36.482342Z","iopub.execute_input":"2023-09-19T16:13:36.482751Z","iopub.status.idle":"2023-09-19T16:13:36.492680Z","shell.execute_reply.started":"2023-09-19T16:13:36.482705Z","shell.execute_reply":"2023-09-19T16:13:36.491501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.read_csv('/kaggle/input/commonlit-deberta-base/oof_df.csv')\n#oof_df = pd.read_csv('/kaggle/input/commonlit-deberta-hidden-layers-mean/oof_df.csv')\nprint(oof_df.shape)\noof_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:13:57.004355Z","iopub.execute_input":"2023-09-19T16:13:57.005243Z","iopub.status.idle":"2023-09-19T16:13:57.043262Z","shell.execute_reply.started":"2023-09-19T16:13:57.005199Z","shell.execute_reply":"2023-09-19T16:13:57.042348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = score_loss(np.array(oof_df[['content' , 'wording']]) , np.array(oof_df[['pred_content' , 'pred_wording']]))\ns","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:14:03.049514Z","iopub.execute_input":"2023-09-19T16:14:03.050013Z","iopub.status.idle":"2023-09-19T16:14:03.094397Z","shell.execute_reply.started":"2023-09-19T16:14:03.049972Z","shell.execute_reply":"2023-09-19T16:14:03.093240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoModel, AutoConfig, \n    AutoTokenizer, logging\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:14:05.782111Z","iopub.execute_input":"2023-09-19T16:14:05.782593Z","iopub.status.idle":"2023-09-19T16:14:05.788957Z","shell.execute_reply.started":"2023-09-19T16:14:05.782555Z","shell.execute_reply":"2023-09-19T16:14:05.787795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\nclass cfg:\n    select = 'base'\n    model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n    only_model_name = f'deberta-v3-{select}'\n    fold = 4\n    batch_size = 32\n    freezing = False\n    max_len = 1024\n    pooling = 'GemText'\n    path = '/kaggle/input/commonlit-deberta-base'\n    #path = '/kaggle/input/commonlit-deberta-hidden-layers-mean/'\n    #path = '/kaggle/input/commonlit-baseline/'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    seed = 42\ncfg.tokenizer = AutoTokenizer.from_pretrained(cfg.path+'/tokenizer/')\ncfg.device , cfg.model_name","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:14:50.655708Z","iopub.execute_input":"2023-09-19T16:14:50.656132Z","iopub.status.idle":"2023-09-19T16:14:51.073717Z","shell.execute_reply.started":"2023-09-19T16:14:50.656100Z","shell.execute_reply":"2023-09-19T16:14:51.071823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_logger(filename='Inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(cfg.seed)\n\n\nLOGGER.info(f\"max_len: {cfg.max_len}\")\nLOGGER.info(f\"batch_size: {cfg.batch_size}\")\nLOGGER.info(f\"Model name: {cfg.only_model_name}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:14:56.229223Z","iopub.execute_input":"2023-09-19T16:14:56.229603Z","iopub.status.idle":"2023-09-19T16:14:56.251575Z","shell.execute_reply.started":"2023-09-19T16:14:56.229574Z","shell.execute_reply":"2023-09-19T16:14:56.250404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsummary_test = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\nsubmission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n\nprint(f\"Prompts Test shape: {prompts_test.shape}\")\ndisplay(prompts_test.head())\nprint(f\"Summary Test shape: {summary_test.shape}\")\ndisplay(summary_test.head())\nprint(f\"Submission shape: {submission.shape}\")\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:14:58.604493Z","iopub.execute_input":"2023-09-19T16:14:58.604904Z","iopub.status.idle":"2023-09-19T16:14:58.648560Z","shell.execute_reply.started":"2023-09-19T16:14:58.604870Z","shell.execute_reply":"2023-09-19T16:14:58.647556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = prompts_test.merge(summary_test, on=\"prompt_id\")\ntest","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:00.989098Z","iopub.execute_input":"2023-09-19T16:15:00.989486Z","iopub.status.idle":"2023-09-19T16:15:01.014471Z","shell.execute_reply.started":"2023-09-19T16:15:00.989451Z","shell.execute_reply":"2023-09-19T16:15:01.013173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['full_text']=test['prompt_question'] +\" \" + cfg.tokenizer.sep_token +\" \"+test['text']\ntest['full_text']","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:02.982852Z","iopub.execute_input":"2023-09-19T16:15:02.983244Z","iopub.status.idle":"2023-09-19T16:15:02.994938Z","shell.execute_reply.started":"2023-09-19T16:15:02.983204Z","shell.execute_reply":"2023-09-19T16:15:02.993910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def odd_layer_freeze(module):\n    for i in range(1,24,2):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n            \ndef even_layer_freeze(module):\n    for i in range(0,24,2):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n            \ndef top_half_layer_freeze(module):\n    for i in range(0,13,1):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False\n\ndef bottom_half_layer_freeze(module):\n    for i in range(13,14,1):\n        for n,p in module.encoder.layer[i].named_parameters():\n            p.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:05.054924Z","iopub.execute_input":"2023-09-19T16:15:05.055343Z","iopub.status.idle":"2023-09-19T16:15:05.063170Z","shell.execute_reply.started":"2023-09-19T16:15:05.055311Z","shell.execute_reply":"2023-09-19T16:15:05.062183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n    \nclass GeMText(nn.Module):\n    def __init__(self, dim = 1, p=3, eps=1e-6):\n        super(GeMText, self).__init__()\n        self.dim = dim\n        self.p = Parameter(torch.ones(1) * p)\n        self.eps = eps\n        self.feat_mult = 1\n\n    def forward(self, last_hidden_state, attention_mask):\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret\n\n\n\ndef get_pooling_layer():\n    if cfg.pooling == 'Mean':\n        return MeanPooling()\n    \n    elif cfg.pooling == 'Max':\n        return MaxPooling()\n    \n    elif cfg.pooling == 'MeanMax':\n        return MeanMaxPooling()\n    \n    elif cfg.pooling == 'GemText':\n        return GeMText()\n\n\nprint(get_pooling_layer())","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:07.496571Z","iopub.execute_input":"2023-09-19T16:15:07.496943Z","iopub.status.idle":"2023-09-19T16:15:07.533645Z","shell.execute_reply.started":"2023-09-19T16:15:07.496913Z","shell.execute_reply":"2023-09-19T16:15:07.532639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaselineModel(nn.Module):\n    def __init__(self, model_name ):\n        super(BaselineModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(cfg.model_name)\n        self.config = AutoConfig.from_pretrained(cfg.model_name)\n        #self.drop = nn.Dropout(p=0.2)\n        self.pooler = get_pooling_layer()\n\n        if cfg.pooling == 'MeanMax':\n            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n        else:\n            self.fc = nn.Linear(self.config.hidden_size, 2)\n            \n        \n        self._init_weights(self.fc)\n        \n        if cfg.freezing:\n            top_half_layer_freeze(self.model)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n           \n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.pooler(out.last_hidden_state, mask)\n        #out = self.drop(out)\n        outputs = self.fc(out)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:12.415270Z","iopub.execute_input":"2023-09-19T16:15:12.415781Z","iopub.status.idle":"2023-09-19T16:15:12.435023Z","shell.execute_reply.started":"2023-09-19T16:15:12.415710Z","shell.execute_reply":"2023-09-19T16:15:12.434028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nclass deberta_hs_Mean(nn.Module):\n    def __init__(self, model_name ):\n        super(deberta_hs_Mean, self).__init__()\n        \n        self.model_config = AutoConfig.from_pretrained(cfg.model_name)\n        self.model_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0,\n                \"add_pooling_layer\": False,\n                \"num_labels\": 2,\n                 \"attention_probs_dropout_prob\":0.0 \n            }\n        )\n        self.model = AutoModel.from_pretrained(cfg.model_name, config=self.model_config)\n        \n        self.fc = nn.Linear(self.model_config.hidden_size, 2)\n        \n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n    def forward(self, ids, mask):\n        outputs = self.model(input_ids=ids,attention_mask=mask)\n        \n        h1 = outputs[1][-1][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h2 = outputs[1][-2][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h3 = outputs[1][-3][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h4 = outputs[1][-4][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h5 = outputs[1][-5][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h6 = outputs[1][-6][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h7 = outputs[1][-7][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h8 = outputs[1][-8][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h9 = outputs[1][-9][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h10 = outputs[1][-10][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h11 = outputs[1][-11][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        h12 = outputs[1][-12][:,0,:].reshape(-1,1,self.model_config.hidden_size)\n        \n        all_cat_mean = torch.mean(\n        torch.cat([ h1, h2, h3, h4, h5, h6,h7,h8,h9,h10,h11,h12], 1)\n        ,1).reshape(-1,1,self.model_config.hidden_size)\n        \n        seq_out = torch.mean(torch.cat([ outputs.last_hidden_state[:,0,:].reshape(-1,1,self.model_config.hidden_size),\n        all_cat_mean] , 1),1)\n        \n        out = self.fc(seq_out)\n        return out\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \nclass BaselineModel(nn.Module):\n    def __init__(self, model_name):\n        super(BaselineModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(model_name)\n        self.config = AutoConfig.from_pretrained(model_name)\n        #self.drop = nn.Dropout(p=0.2)\n        self.pooler = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 2)\n        \n        self._init_weights(self.fc)        \n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n           \n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.pooler(out.last_hidden_state, mask)\n        #out = self.drop(out)\n        outputs = self.fc(out)\n        return outputs\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self,df):\n        self.df = df\n        self.tokenizer = cfg.tokenizer\n        self.max_len = cfg.max_len\n        self.pq = df['prompt_question'].values\n        self.pt = df['prompt_title'].values\n        self.text = df['text'].values\n        self.title = df['prompt_title'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self , index):\n        pq   =   self.pq[index]\n        text =   self.text[index]\n        pt = self.pt[index]\n        # full_text = pq+\" \" + self.tokenizer.sep_token +\" \"+text\n        # full_text = pt +\" \" + self.tokenizer.sep_token +\" \"+ pq + \" \" + self.tokenizer.sep_token + \" \" +text\n        full_text=text\n        \n        inputs = self.tokenizer.encode_plus(\n                        full_text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                        \n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        \n        return {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'attention_mask': torch.tensor(mask, dtype=torch.long)\n        }\n    \ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:, :mask_len]\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:18.771475Z","iopub.execute_input":"2023-09-19T16:15:18.771819Z","iopub.status.idle":"2023-09-19T16:15:18.786137Z","shell.execute_reply.started":"2023-09-19T16:15:18.771791Z","shell.execute_reply":"2023-09-19T16:15:18.785113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef test_run(model , loader):\n    \n    \n    model.eval()\n       \n    preds = []\n    bar = tqdm(enumerate(loader), total=len(loader))\n    for idx , data in bar:\n        inputs = collate(data)\n        ids   =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n        mask  =  inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n        y_preds = model(ids , mask)\n        preds.append(y_preds.to('cpu').numpy())\n    \n    predictions = np.concatenate(preds)\n    \n    return predictions\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:21.619633Z","iopub.execute_input":"2023-09-19T16:15:21.620049Z","iopub.status.idle":"2023-09-19T16:15:21.635324Z","shell.execute_reply.started":"2023-09-19T16:15:21.620015Z","shell.execute_reply":"2023-09-19T16:15:21.634354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(test)\ntest_loader = DataLoader(test_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:24.102299Z","iopub.execute_input":"2023-09-19T16:15:24.102778Z","iopub.status.idle":"2023-09-19T16:15:24.109779Z","shell.execute_reply.started":"2023-09-19T16:15:24.102738Z","shell.execute_reply":"2023-09-19T16:15:24.108709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfor fold in range(cfg.fold):\n    print('******** fold' , fold , '********')\n    \n    model  = BaselineModel(cfg.model_name).to(cfg.device)\n    model.load_state_dict(torch.load(f\"/kaggle/input/commonlit-deberta-base/deberta-v3-{cfg.select}_Fold_{fold}.pth\", map_location=torch.device('cpu')))\n    #model.load_state_dict(torch.load(f\"/kaggle/input/commonlit-deberta-hidden-layers-mean/deberta-v3-base_Fold_{fold}.pth\", map_location=torch.device('cpu')))\n    preds = test_run(model, test_loader)\n    final_preds.append(preds)\n    del model ; gc.collect()\n    torch.cuda.empty_cache()\n    # /kaggle/input/commonlit-deberta-base/deberta-v3-large_Fold_0.pth\nfinal_preds_ = np.mean(final_preds, axis=0)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:15:57.750537Z","iopub.execute_input":"2023-09-19T16:15:57.750954Z","iopub.status.idle":"2023-09-19T16:16:41.600824Z","shell.execute_reply.started":"2023-09-19T16:15:57.750919Z","shell.execute_reply":"2023-09-19T16:16:41.599563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:16:45.417184Z","iopub.execute_input":"2023-09-19T16:16:45.418043Z","iopub.status.idle":"2023-09-19T16:16:45.437167Z","shell.execute_reply.started":"2023-09-19T16:16:45.418006Z","shell.execute_reply":"2023-09-19T16:16:45.435675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_cols=['content','wording']\ntest[target_cols] = final_preds_\nsubmission = submission.drop(columns=target_cols).merge(test[['student_id'] + target_cols], on='student_id', how='left')\ndisplay(submission.head())\nsubmission[['student_id'] + target_cols].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T16:16:48.269749Z","iopub.execute_input":"2023-09-19T16:16:48.270479Z","iopub.status.idle":"2023-09-19T16:16:48.304220Z","shell.execute_reply.started":"2023-09-19T16:16:48.270408Z","shell.execute_reply":"2023-09-19T16:16:48.303027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}