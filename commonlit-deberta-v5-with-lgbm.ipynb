{"cells":[{"cell_type":"code","execution_count":49,"metadata":{"papermill":{"duration":18.376581,"end_time":"2023-08-05T11:57:47.2689","exception":false,"start_time":"2023-08-05T11:57:28.892319","status":"completed"},"tags":[]},"outputs":[],"source":["\n","import os\n","import gc\n","from tqdm.auto import tqdm\n","import transformers\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.metrics import mean_squared_error\n","from plotly.subplots import make_subplots #graphing\n","import plotly.express as px #graphing\n","import plotly.graph_objects as go #graphing\n","import plotly.figure_factory as ff #graphing\n","\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","from datasets import concatenate_datasets,load_dataset,load_from_disk\n","from sklearn.metrics import log_loss\n","from transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n","from transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n","from transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import lr_scheduler\n","from torch.nn.parameter import Parameter\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","from spellchecker import SpellChecker\n","from nltk.corpus import stopwords\n","\n","\n","import time\n","import warnings\n","import collections\n","import spacy\n","import re\n","# from termcolor import colored\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\"\"\"import nltk\n","nltk.download('stopwords')\"\"\"\n","\n","\"\"\"import en_core_web_sm\n","nlp = en_core_web_sm.load()\"\"\"\n","tqdm.pandas()"]},{"cell_type":"code","execution_count":3,"metadata":{"papermill":{"duration":0.016027,"end_time":"2023-08-05T11:57:47.293377","exception":false,"start_time":"2023-08-05T11:57:47.27735","status":"completed"},"tags":[]},"outputs":[],"source":["OUTPUT_DIR = './'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)"]},{"cell_type":"code","execution_count":4,"metadata":{"papermill":{"duration":0.015833,"end_time":"2023-08-05T11:57:47.3175","exception":false,"start_time":"2023-08-05T11:57:47.301667","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import (\n","    AutoModel, AutoConfig, \n","    AutoTokenizer, logging\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"papermill":{"duration":0.055438,"end_time":"2023-08-05T11:57:47.381677","exception":false,"start_time":"2023-08-05T11:57:47.326239","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["=========================== Model name :deberta-v3-base ===========================: \n","\n","\n","Scheduler: CosineAnnealingLR\n","batch_size: 4 with gradient Accumukation 16 \n","Pooling name: GemText \n","Freezing: False\n","Max Length: 512\n","Num Epochs: 1\n","\n","\n"]}],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","\n","\n","class cfg:\n","    select = 'base'\n","    #model_name = f'/kaggle/input/deberta-v3-{select}/deberta-v3-{select}'\n","    model_name = f'./Models/deberta-v3-{select}'\n","\n","    only_model_name = f'deberta-v3-{select}'\n","    accum_iter = 16\n","    fold = 4\n","    split = 5\n","    seed = 42\n","    batch_size = 4\n","    max_len = 512\n","    num_epoch = 1\n","    T_max= 500\n","    \n","    scheduler = 'CosineAnnealingLR'\n","    weight_decay =  1e-6\n","    min_lr = 1e-6\n","    freezing = False\n","    pooling = 'GemText'\n","    weight_decay = 1e-2\n","    encoder_lr = 1e-5\n","    decoder_lr = 1e-5\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]#.detach().to('cpu').numpy()\n","        y_pred = y_preds[:,i]#.detach().to('cpu').numpy()\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","def score_loss(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return {\n","        'mcrmse_score' : mcrmse_score,\n","        'Content_score' : scores[0],\n","        'Wording_score' : scores[1]\n","    }\n","\n","def get_logger(filename='Training'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","def set_seed(seed=42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","set_seed(cfg.seed)\n","\n","\n","LOGGER.info(f\"=========================== Model name :{cfg.only_model_name} ===========================: \")\n","LOGGER.info('\\n')\n","LOGGER.info(f\"Scheduler: {cfg.scheduler}\")\n","LOGGER.info(f\"batch_size: {cfg.batch_size} with gradient Accumukation {cfg.accum_iter} \")\n","LOGGER.info(f\"Pooling name: {cfg.pooling} \")\n","LOGGER.info(f\"Freezing: {cfg.freezing}\")\n","LOGGER.info(f\"Max Length: {cfg.max_len}\")\n","LOGGER.info(f\"Num Epochs: {cfg.num_epoch}\")\n","LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":6,"metadata":{"papermill":{"duration":0.138391,"end_time":"2023-08-05T11:57:47.528988","exception":false,"start_time":"2023-08-05T11:57:47.390597","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt Train.shape: (4, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt_id</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39c16e</td>\n","      <td>Summarize at least 3 elements of an ideal trag...</td>\n","      <td>On Tragedy</td>\n","      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3b9047</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>814d6b</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ebad26</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  prompt_id                                    prompt_question  \\\n","0    39c16e  Summarize at least 3 elements of an ideal trag...   \n","1    3b9047  In complete sentences, summarize the structure...   \n","2    814d6b  Summarize how the Third Wave developed over su...   \n","3    ebad26  Summarize the various ways the factory would u...   \n","\n","                prompt_title  \\\n","0                 On Tragedy   \n","1  Egyptian Social Structure   \n","2             The Third Wave   \n","3    Excerpt from The Jungle   \n","\n","                                         prompt_text  \n","0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n","1  Egyptian society was structured like a pyramid...  \n","2  Background \\r\\nThe Third Wave experiment took ...  \n","3  With one member trimming beef in a cannery, an...  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Summary Train.shape: (7165, 5)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  \n","0  0.205683  0.380538  \n","1 -0.548304  0.506755  \n","2  3.128928  4.231226  \n","3 -0.210614 -0.471415  \n","4  3.272894  3.219757  "]},"metadata":{},"output_type":"display_data"}],"source":["\"\"\"train_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n","test_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n","submission = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv')\n","train_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n","test_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\"\"\"\n","\n","train_prompts = pd.read_csv('./Data/prompts_train.csv')\n","test_prompts = pd.read_csv('./Data/prompts_test.csv')\n","submission = pd.read_csv('./Data/sample_submission.csv')\n","train_data = pd.read_csv('./Data/summaries_train.csv')\n","test_data = pd.read_csv('./Data/summaries_test.csv')\n","\n","print(f\"Prompt Train.shape: {train_prompts.shape}\")\n","display(train_prompts.head())\n","print(f\"Summary Train.shape: {train_data.shape}\")\n","display(train_data.head())"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["class Preprocessor:\n","    def __init__(self, \n","                model_name: str,\n","                ) -> None:\n","        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","        self.STOP_WORDS = set(stopwords.words('english'))\n","        \n","        self.spacy_ner_model = spacy.load('en_core_web_sm',)\n","        self.speller = SpellChecker() #Speller(lang='en')\n","        \n","    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n","        \"\"\" text length \"\"\"\n","        tokenizer=self.tokenizer\n","        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n","\n","    def word_overlap_count(self, row):\n","        \"\"\" intersection(prompt_text, text) \"\"\"        \n","        def check_is_stop_word(word):\n","            return word in self.STOP_WORDS\n","        \n","        prompt_words = row['prompt_tokens']\n","        summary_words = row['summary_tokens']\n","        if self.STOP_WORDS:\n","            prompt_words = list(filter(check_is_stop_word, prompt_words))\n","            summary_words = list(filter(check_is_stop_word, summary_words))\n","        return len(set(prompt_words).intersection(set(summary_words)))\n","            \n","    def ngrams(self, token, n):\n","        # Use the zip function to help us generate n-grams\n","        # Concatentate the tokens into ngrams and return\n","        ngrams = zip(*[token[i:] for i in range(n)])\n","        return [\" \".join(ngram) for ngram in ngrams]\n","\n","    def ngram_co_occurrence(self, row, n: int):\n","        # Tokenize the original text and summary into words\n","        original_tokens = row['prompt_tokens']\n","        summary_tokens = row['summary_tokens']\n","\n","        # Generate n-grams for the original text and summary\n","        original_ngrams = set(self.ngrams(original_tokens, n))\n","        summary_ngrams = set(self.ngrams(summary_tokens, n))\n","\n","        # Calculate the number of common n-grams\n","        common_ngrams = original_ngrams.intersection(summary_ngrams)\n","\n","        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n","        # original_ngram_freq = Counter(ngrams(original_words, n))\n","        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n","        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n","\n","        return len(common_ngrams)\n","    \n","    def ner_overlap_count(self, row, mode:str):\n","        model = self.spacy_ner_model\n","        def clean_ners(ner_list):\n","            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n","        prompt = model(row['prompt_text'])\n","        summary = model(row['text'])\n","\n","        if \"spacy\" in str(model):\n","            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n","        elif \"stanza\" in str(model):\n","            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n","            summary_ner = set([(token.text, token.type) for token in summary.ents])\n","        else:\n","            raise Exception(\"Model not supported\")\n","\n","        prompt_ner = clean_ners(prompt_ner)\n","        summary_ner = clean_ners(summary_ner)\n","        intersecting_ners = prompt_ner.intersection(summary_ner)\n","        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n","        \n","        if mode == \"train\":\n","            return ner_dict\n","        elif mode == \"test\":\n","            return {key: ner_dict.get(key) for key in self.ner_keys}\n"," \n","    def quotes_count(self, row):\n","        summary = row['text']\n","        text = row['prompt_text']\n","        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n","        if len(quotes_from_summary)>0:\n","            return [quote in text for quote in quotes_from_summary].count(True)\n","        else:\n","            return 0\n","\n","    def spelling(self, text):\n","        \n","        wordlist=text.split()\n","        amount_miss = len(list(self.speller.unknown(wordlist)))\n","\n","        return amount_miss\n","    \n","    def run(self, \n","            prompts: pd.DataFrame,\n","            summaries:pd.DataFrame,\n","            mode:str\n","        ) -> pd.DataFrame:\n","        \n","        # before merge preprocess\n","        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: len(self.tokenizer.encode(x))\n","        )\n","        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n","            lambda x: self.tokenizer.convert_ids_to_tokens(\n","                self.tokenizer.encode(x), \n","                skip_special_tokens=True\n","            )\n","        )\n","\n","        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n","            lambda x: len(self.tokenizer.encode(x))\n","        )\n","        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n","            lambda x: self.tokenizer.convert_ids_to_tokens(\n","                self.tokenizer.encode(x), \n","                skip_special_tokens=True\n","            )\n","\n","        )\n","        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n","\n","        # merge prompts and summaries\n","        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n","\n","        # after merge preprocess\n","        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n","        \n","        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n","        input_df['bigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence,args=(2,), axis=1 \n","        )\n","        input_df['trigram_overlap_count'] = input_df.progress_apply(\n","            self.ngram_co_occurrence, args=(3,), axis=1\n","        )\n","        \n","        # Crate dataframe with count of each category NERs overlap for all the summaries\n","        # Because it spends too much time for this feature, I don't use this time.\n","#         ners_count_df  = input_df.progress_apply(\n","#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n","#         ).fillna(0)\n","#         self.ner_keys = ners_count_df.columns\n","#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n","#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n","#         # join ner count dataframe with train dataframe\n","#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n","        \n","        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n","        \n","        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n","    \n","preprocessor = Preprocessor(model_name=cfg.model_name)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7165/7165 [00:00<00:00, 15153.21it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 26056.12it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 9803.03it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 8460.32it/s]\n","100%|██████████| 7165/7165 [00:00<00:00, 169679.96it/s]\n","100%|██████████| 4/4 [00:00<00:00, 18808.54it/s]\n","100%|██████████| 4/4 [00:00<00:00, 4403.47it/s]\n","100%|██████████| 4/4 [00:00<00:00, 5475.59it/s]\n","100%|██████████| 4/4 [00:00<00:00, 6743.25it/s]\n","100%|██████████| 4/4 [00:00<00:00, 5332.87it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>summary_length</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>length_ratio</th>\n","      <th>word_overlap_count</th>\n","      <th>bigram_overlap_count</th>\n","      <th>trigram_overlap_count</th>\n","      <th>quotes_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000000ffffff</td>\n","      <td>abc123</td>\n","      <td>Example text 1</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>7</td>\n","      <td>0.714286</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>111111eeeeee</td>\n","      <td>def789</td>\n","      <td>Example text 2</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>7</td>\n","      <td>0.714286</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>222222cccccc</td>\n","      <td>abc123</td>\n","      <td>Example text 3</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 1</td>\n","      <td>Heading\\nText...</td>\n","      <td>7</td>\n","      <td>0.714286</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333333dddddd</td>\n","      <td>def789</td>\n","      <td>Example text 4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>Summarize...</td>\n","      <td>Example Title 2</td>\n","      <td>Heading\\nText...</td>\n","      <td>7</td>\n","      <td>0.714286</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id            text  summary_length  splling_err_num  \\\n","0  000000ffffff    abc123  Example text 1               5                0   \n","1  111111eeeeee    def789  Example text 2               5                0   \n","2  222222cccccc    abc123  Example text 3               5                0   \n","3  333333dddddd    def789  Example text 4               5                0   \n","\n","  prompt_question     prompt_title       prompt_text  prompt_length  \\\n","0    Summarize...  Example Title 1  Heading\\nText...              7   \n","1    Summarize...  Example Title 2  Heading\\nText...              7   \n","2    Summarize...  Example Title 1  Heading\\nText...              7   \n","3    Summarize...  Example Title 2  Heading\\nText...              7   \n","\n","   length_ratio  word_overlap_count  bigram_overlap_count  \\\n","0      0.714286                   0                     0   \n","1      0.714286                   0                     0   \n","2      0.714286                   0                     0   \n","3      0.714286                   0                     0   \n","\n","   trigram_overlap_count  quotes_count  \n","0                      0             0  \n","1                      0             0  \n","2                      0             0  \n","3                      0             0  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_data = preprocessor.run(train_prompts, train_data, mode=\"train\")\n","test_data = preprocessor.run(test_prompts, test_data, mode=\"test\")\n","\n","test_data.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>length_ratio</th>\n","      <th>word_overlap_count</th>\n","      <th>bigram_overlap_count</th>\n","      <th>trigram_overlap_count</th>\n","      <th>quotes_count</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>69</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.102832</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>56</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>1137</td>\n","      <td>0.049252</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>285</td>\n","      <td>32</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.437788</td>\n","      <td>1</td>\n","      <td>56</td>\n","      <td>26</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>43</td>\n","      <td>5</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.066052</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>253</td>\n","      <td>29</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.377049</td>\n","      <td>1</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  summary_length  splling_err_num  \\\n","0  0.205683  0.380538              69                5   \n","1 -0.548304  0.506755              56                2   \n","2  3.128928  4.231226             285               32   \n","3 -0.210614 -0.471415              43                5   \n","4  3.272894  3.219757             253               29   \n","\n","                                     prompt_question  \\\n","0  Summarize how the Third Wave developed over su...   \n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","3  In complete sentences, summarize the structure...   \n","4  Summarize how the Third Wave developed over su...   \n","\n","                prompt_title  \\\n","0             The Third Wave   \n","1    Excerpt from The Jungle   \n","2  Egyptian Social Structure   \n","3  Egyptian Social Structure   \n","4             The Third Wave   \n","\n","                                         prompt_text  prompt_length  \\\n","0  Background \\r\\nThe Third Wave experiment took ...            671   \n","1  With one member trimming beef in a cannery, an...           1137   \n","2  Egyptian society was structured like a pyramid...            651   \n","3  Egyptian society was structured like a pyramid...            651   \n","4  Background \\r\\nThe Third Wave experiment took ...            671   \n","\n","   length_ratio  word_overlap_count  bigram_overlap_count  \\\n","0      0.102832                   0                     5   \n","1      0.049252                   0                    22   \n","2      0.437788                   1                    56   \n","3      0.066052                   1                    10   \n","4      0.377049                   1                    27   \n","\n","   trigram_overlap_count  quotes_count  fold  \n","0                      0             0   3.0  \n","1                     10             0   2.0  \n","2                     26             2   1.0  \n","3                      6             0   1.0  \n","4                      5             4   3.0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"fold = StratifiedKFold(n_splits=cfg.fold, shuffle=True, random_state=cfg.seed)\n","for n, (train_index, val_index) in enumerate(fold.split(train, train['prompt_id'])):\n","    train.loc[val_index, 'fold'] = n\n","train['fold'] = train['fold'].astype(int)\n","fold_sizes = train.groupby('fold').size()\n","print(fold_sizes)\"\"\"\n","\n","gkf = GroupKFold(n_splits = cfg.fold)\n","\n","for i, (_, val_index) in enumerate(gkf.split(train_data, groups = train_data['prompt_id'])):\n","    train_data.loc[val_index, 'fold'] = i\n","    \n","train_data.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"papermill":{"duration":0.308297,"end_time":"2023-08-05T11:57:48.051785","exception":false,"start_time":"2023-08-05T11:57:47.743488","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(647, 27, 966)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["max_words_text = train_data[\"text\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_question = train_prompts[\"prompt_question\"].apply(lambda x: len(x.split())).max()\n","max_words_prompt_text = train_prompts[\"prompt_text\"].apply(lambda x: len(x.split())).max()\n","\n","## max words\n","max_words_text, max_words_prompt_question, max_words_prompt_text"]},{"cell_type":"code","execution_count":11,"metadata":{"papermill":{"duration":1.482642,"end_time":"2023-08-05T11:57:49.545675","exception":false,"start_time":"2023-08-05T11:57:48.063033","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["DebertaV2TokenizerFast(name_or_path='./Models/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","cfg.tokenizer = tokenizer\n","cfg.tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{"papermill":{"duration":0.021197,"end_time":"2023-08-05T11:57:49.578152","exception":false,"start_time":"2023-08-05T11:57:49.556955","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>length_ratio</th>\n","      <th>word_overlap_count</th>\n","      <th>bigram_overlap_count</th>\n","      <th>trigram_overlap_count</th>\n","      <th>quotes_count</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>69</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.102832</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>56</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>1137</td>\n","      <td>0.049252</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>285</td>\n","      <td>32</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.437788</td>\n","      <td>1</td>\n","      <td>56</td>\n","      <td>26</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>43</td>\n","      <td>5</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.066052</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>253</td>\n","      <td>29</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.377049</td>\n","      <td>1</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  summary_length  splling_err_num  \\\n","0  0.205683  0.380538              69                5   \n","1 -0.548304  0.506755              56                2   \n","2  3.128928  4.231226             285               32   \n","3 -0.210614 -0.471415              43                5   \n","4  3.272894  3.219757             253               29   \n","\n","                                     prompt_question  \\\n","0  Summarize how the Third Wave developed over su...   \n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","3  In complete sentences, summarize the structure...   \n","4  Summarize how the Third Wave developed over su...   \n","\n","                prompt_title  \\\n","0             The Third Wave   \n","1    Excerpt from The Jungle   \n","2  Egyptian Social Structure   \n","3  Egyptian Social Structure   \n","4             The Third Wave   \n","\n","                                         prompt_text  prompt_length  \\\n","0  Background \\r\\nThe Third Wave experiment took ...            671   \n","1  With one member trimming beef in a cannery, an...           1137   \n","2  Egyptian society was structured like a pyramid...            651   \n","3  Egyptian society was structured like a pyramid...            651   \n","4  Background \\r\\nThe Third Wave experiment took ...            671   \n","\n","   length_ratio  word_overlap_count  bigram_overlap_count  \\\n","0      0.102832                   0                     5   \n","1      0.049252                   0                    22   \n","2      0.437788                   1                    56   \n","3      0.066052                   1                    10   \n","4      0.377049                   1                    27   \n","\n","   trigram_overlap_count  quotes_count  fold  \n","0                      0             0   3.0  \n","1                     10             0   2.0  \n","2                     26             2   1.0  \n","3                      6             0   1.0  \n","4                      5             4   3.0  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_df = train_data\n","train_df.head()"]},{"cell_type":"code","execution_count":13,"metadata":{"papermill":{"duration":0.024017,"end_time":"2023-08-05T11:57:49.680128","exception":false,"start_time":"2023-08-05T11:57:49.656111","status":"completed"},"tags":[]},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = cfg.tokenizer\n","        self.max_len = cfg.max_len\n","        self.pq = df['prompt_question'].values\n","        self.pt = df['prompt_title'].values\n","        self.text = df['text'].values\n","        self.targets = df[['content' , 'wording']].values\n","        self.stuid = df['student_id'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self , index):\n","        pq   =   self.pq[index]\n","        text =   self.text[index]\n","        pt = self.pt[index]\n","        # full_text = pq +\" \" + self.tokenizer.sep_token +\" \"+text\n","        full_text = pt +\" \" + self.tokenizer.sep_token +\" \"+ pq + \" \" + self.tokenizer.sep_token + \" \" +text\n","        # full_text = text\n","        \n","        inputs = self.tokenizer.encode_plus(\n","                        full_text,\n","                        truncation=True,\n","                        add_special_tokens=True,\n","                        max_length=self.max_len,\n","                        padding='max_length'\n","                        \n","                    )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        target = self.targets[index]\n","        stuid = self.stuid[index]\n","        \n","   \n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            \n","        } , torch.tensor(target, dtype=torch.float), stuid\n","\n","def collate(inputs):\n","    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:, :mask_len]\n","    return inputs\n"]},{"cell_type":"code","execution_count":14,"metadata":{"papermill":{"duration":0.020298,"end_time":"2023-08-05T11:57:49.712299","exception":false,"start_time":"2023-08-05T11:57:49.692001","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["GeMText()\n"]}],"source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class MaxPooling(nn.Module):\n","    def __init__(self):\n","        super(MaxPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        embeddings = last_hidden_state.clone()\n","        embeddings[input_mask_expanded == 0] = -1e9\n","        max_embeddings, _ = torch.max(embeddings, dim = 1)\n","        return max_embeddings\n","\n","class MeanMax(nn.Module):\n","    def __init__(self):\n","        super(MeanMax, self).__init__()\n","        \n","        self.mean_pooler = MeanPooling()\n","        self.max_pooler  = MaxPooling()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        mean_pooler = self.mean_pooler( last_hidden_state ,attention_mask )\n","        max_pooler =  self.max_pooler( last_hidden_state ,attention_mask )\n","        out = torch.concat([mean_pooler ,max_pooler ] , 1)\n","        return out\n","\n","class GeMText(nn.Module):\n","    def __init__(self, dim = 1, p=3, eps=1e-6):\n","        super(GeMText, self).__init__()\n","        self.dim = dim\n","        self.p = Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","        self.feat_mult = 1\n","\n","    def forward(self, last_hidden_state, attention_mask):\n","        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n","        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n","        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n","        ret = ret.pow(1 / self.p)\n","        return ret\n","\n","def get_pooling_layer():\n","    if cfg.pooling == 'Mean':\n","        return MeanPooling()\n","    \n","    elif cfg.pooling == 'Max':\n","        return MaxPooling()\n","    \n","    elif cfg.pooling == 'MeanMax':\n","        return MeanMax()\n","    \n","    elif cfg.pooling == 'GemText':\n","        return GeMText()\n","\n","\n","print(get_pooling_layer())"]},{"cell_type":"code","execution_count":15,"metadata":{"papermill":{"duration":0.022006,"end_time":"2023-08-05T11:57:49.932506","exception":false,"start_time":"2023-08-05T11:57:49.9105","status":"completed"},"tags":[]},"outputs":[],"source":["def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","    \n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","\n","def odd_layer_freeze(module):\n","    for i in range(1,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def even_layer_freeze(module):\n","    for i in range(0,24,2):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","            \n","def top_half_layer_freeze(module):\n","    for i in range(0,13,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","\n","def bottom_half_layer_freeze(module):\n","    for i in range(13,14,1):\n","        for n,p in module.encoder.layer[i].named_parameters():\n","            p.requires_grad = False\n","        "]},{"cell_type":"code","execution_count":16,"metadata":{"papermill":{"duration":0.020541,"end_time":"2023-08-05T11:57:49.964752","exception":false,"start_time":"2023-08-05T11:57:49.944211","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'\\n## Check layers which one are freeze \\nfor n,p in model.named_parameters():\\n    print(n,p.requires_grad)\\n'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["\n","'''\n","## Check layers which one are freeze \n","for n,p in model.named_parameters():\n","    print(n,p.requires_grad)\n","'''"]},{"cell_type":"code","execution_count":17,"metadata":{"papermill":{"duration":0.018841,"end_time":"2023-08-05T11:57:49.995581","exception":false,"start_time":"2023-08-05T11:57:49.97674","status":"completed"},"tags":[]},"outputs":[],"source":["\n","#if cfg.freezing:\n","#    top_half_layer_freeze(model)"]},{"cell_type":"code","execution_count":18,"metadata":{"papermill":{"duration":0.026171,"end_time":"2023-08-05T11:57:50.033939","exception":false,"start_time":"2023-08-05T11:57:50.007768","status":"completed"},"tags":[]},"outputs":[],"source":["class BaselineModel(nn.Module):\n","    def __init__(self, model_name ):\n","        super(BaselineModel, self).__init__()\n","        \n","        self.model = AutoModel.from_pretrained(cfg.model_name)\n","        self.config = AutoConfig.from_pretrained(cfg.model_name)\n","        #self.drop = nn.Dropout(p=0.2)\n","        self.pooler = get_pooling_layer()\n","\n","        if cfg.pooling == 'MeanMax':\n","            self.fc = nn.Linear(2*self.config.hidden_size, 2)\n","        else:\n","            self.fc = nn.Linear(self.config.hidden_size, 2)\n","            \n","        \n","        self._init_weights(self.fc)\n","        \n","        if cfg.freezing:\n","            top_half_layer_freeze(self.model)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","           \n","    def forward(self, ids, mask):\n","        out = self.model(input_ids=ids,attention_mask=mask,\n","                         output_hidden_states=False)\n","        out = self.pooler(out.last_hidden_state, mask)\n","        #out = self.drop(out)\n","        outputs = self.fc(out)\n","        return outputs"]},{"cell_type":"code","execution_count":19,"metadata":{"papermill":{"duration":0.024277,"end_time":"2023-08-05T11:57:50.070421","exception":false,"start_time":"2023-08-05T11:57:50.046144","status":"completed"},"tags":[]},"outputs":[],"source":["def train_run(model ,criterion ,optimizer , dataloader):\n","    \n","    model.train()\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    running_loss = 0.0\n","    dataset_size = 0.0 \n","    \n","    \n","    for batch_idx , (data , labels, stuid) in bar:\n","        inputs , target = collate(data) , labels    \n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","        outputs = model(ids, mask)\n","        loss = criterion(outputs, targets)\n","        \n","        # normalize loss to account for batch accumulation\n","        loss = loss / cfg.accum_iter \n","        loss.backward()\n","        \n","        if ((batch_idx + 1) % cfg.accum_iter == 0) or (batch_idx + 1 == len(dataloader)):\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","\n","    epoch_loss = running_loss/dataset_size\n","    gc.collect()\n","    \n","\n","    \n","    return epoch_loss\n"]},{"cell_type":"code","execution_count":20,"metadata":{"papermill":{"duration":0.024256,"end_time":"2023-08-05T11:57:50.107068","exception":false,"start_time":"2023-08-05T11:57:50.082812","status":"completed"},"tags":[]},"outputs":[],"source":["@torch.no_grad()\n","def valid_run(model , dataloader):\n","    model.eval()\n","    \n","    running_loss = 0.0\n","    dataset_size = 0.0\n","    \n","    predictions = []\n","    y_labels = []\n","    stuid_collect = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for batch_idx , (data , labels, stuid) in bar:\n","        inputs , target = collate(data) , labels\n","        ids  =  inputs['input_ids'].to(cfg.device, dtype = torch.long)\n","        mask = inputs['attention_mask'].to(cfg.device, dtype = torch.long)\n","        targets = target.to(cfg.device, dtype = torch.float)\n","        \n","        batch_size = ids.size(0)\n","\n","        outputs = model(ids, mask)\n","        \n","        loss = criterion(outputs, targets)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        predictions.append(outputs.detach().to('cpu').numpy())\n","        y_labels.append(labels.detach().to('cpu').numpy())\n","        stuid_collect.append(stuid)\n","    \n","    predictions = np.concatenate(predictions)\n","    y_labels    = np.concatenate(y_labels)\n","\n","    epoch_loss = running_loss / dataset_size\n","    gc.collect()   \n","    \n","    return epoch_loss , predictions , y_labels, stuid_collect\n","        \n","    "]},{"cell_type":"code","execution_count":21,"metadata":{"papermill":{"duration":0.020672,"end_time":"2023-08-05T11:57:50.139824","exception":false,"start_time":"2023-08-05T11:57:50.119152","status":"completed"},"tags":[]},"outputs":[],"source":["def prepare_fold(fold):\n","    \n","    dftrain = train_df[train_df['fold']!= fold]\n","    dfvalid = train_df[train_df['fold']== fold]\n","    \n","    train_dataset = TrainDataset(dftrain)\n","    valid_dataset = TrainDataset(dfvalid)\n","    \n","    train_loader = DataLoader(train_dataset , batch_size=cfg.batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n","    valid_loader = DataLoader(valid_dataset ,batch_size=cfg.batch_size,num_workers=2, shuffle=True, pin_memory=True)\n","    \n","    return train_loader , valid_loader\n","    "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["train_loader , valid_loader = prepare_fold(0)"]},{"cell_type":"code","execution_count":23,"metadata":{"papermill":{"duration":0.020078,"end_time":"2023-08-05T11:57:50.172036","exception":false,"start_time":"2023-08-05T11:57:50.151958","status":"completed"},"tags":[]},"outputs":[],"source":["def oof_df(stuid , true , pred):\n","    \n","    df_pred = pd.DataFrame(pred ,columns= ['pred_content' , 'pred_wording'] )\n","    df_real = pd.DataFrame(true ,columns= ['content' , 'wording'] )\n","    df_stuid = pd.DataFrame(stuid, columns= ['student_id'])\n","\n","    df = pd.concat([df_stuid, df_real , df_pred ], axis=1)\n","\n","    return df\n"]},{"cell_type":"code","execution_count":24,"metadata":{"papermill":{"duration":0.022134,"end_time":"2023-08-05T11:57:50.206375","exception":false,"start_time":"2023-08-05T11:57:50.184241","status":"completed"},"tags":[]},"outputs":[],"source":["def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","              'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","              'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters"]},{"cell_type":"code","execution_count":32,"metadata":{"papermill":{"duration":10635.997725,"end_time":"2023-08-05T14:55:06.216462","exception":false,"start_time":"2023-08-05T11:57:50.218737","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","\n","========== fold: 0 training ==========\n","Number of batches in Train 1277 and valid 515 dataset\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1277 [00:00<02:20,  9.09it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1277/1277 [01:41<00:00, 13.13it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1277/1277 [01:41<00:00, 12.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 4/515 [00:00<00:13, 38.04it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 510/515 [00:09<00:00, 47.02it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 515/515 [00:10<00:00, 51.29it/s]\n","Validation Loss Improved (inf ---> 0.17481797593648016)\n","Weights and oof values saved for epochs-0 .....\n","Epoch 0 Training Loss 0.0168 Validation Loss 0.1748\n"," Training complete in 0h 1m 54s\n"," Best Loss: 0.1748\n"," oof for fold 0 ---> {'mcrmse_score': 0.6038852, 'Content_score': 0.5531316, 'Wording_score': 0.6546388}\n","\n","\n","\n","\n","========== fold: 1 training ==========\n","Number of batches in Train 1289 and valid 503 dataset\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1289 [00:00<01:11, 17.99it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1288/1289 [01:34<00:00, 13.67it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1289/1289 [01:34<00:00, 13.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 4/503 [00:00<00:14, 34.09it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 500/503 [00:14<00:00, 34.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 503/503 [00:14<00:00, 34.89it/s]\n","Validation Loss Improved (inf ---> 0.4696841659149523)\n","Weights and oof values saved for epochs-0 .....\n","Epoch 0 Training Loss 0.0158 Validation Loss 0.4697\n"," Training complete in 0h 1m 53s\n"," Best Loss: 0.4697\n"," oof for fold 1 ---> {'mcrmse_score': 1.0331246, 'Content_score': 0.95151836, 'Wording_score': 1.1147307}\n","\n","\n","\n","\n","========== fold: 2 training ==========\n","Number of batches in Train 1293 and valid 499 dataset\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1293 [00:00<02:11,  9.82it/s]"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1293/1293 [01:44<00:00, 12.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 5/499 [00:00<00:12, 40.76it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 496/499 [00:12<00:00, 38.33it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 499/499 [00:12<00:00, 39.43it/s]\n","Validation Loss Improved (inf ---> 0.17371372363057905)\n","Weights and oof values saved for epochs-0 .....\n","Epoch 0 Training Loss 0.0169 Validation Loss 0.1737\n"," Training complete in 0h 2m 0s\n"," Best Loss: 0.1737\n"," oof for fold 2 ---> {'mcrmse_score': 0.60316455, 'Content_score': 0.50743043, 'Wording_score': 0.6988987}\n","\n","\n","\n","\n","========== fold: 3 training ==========\n","Number of batches in Train 1516 and valid 276 dataset\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1516 [00:00<01:31, 16.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|█████████▉| 1515/1516 [02:03<00:00, 12.25it/s]"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1516/1516 [02:03<00:00, 12.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 5/276 [00:00<00:05, 45.20it/s]"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 276/276 [00:06<00:00, 44.12it/s]\n","Validation Loss Improved (inf ---> 0.23534190662860438)\n","Weights and oof values saved for epochs-0 .....\n","Epoch 0 Training Loss 0.0166 Validation Loss 0.2353\n"," Training complete in 0h 2m 13s\n"," Best Loss: 0.2353\n"," oof for fold 3 ---> {'mcrmse_score': 0.7188897, 'Content_score': 0.6402134, 'Wording_score': 0.79756606}\n","\n","\n"]}],"source":["oof_dfs = []\n","for n_fold in range(cfg.fold):\n","    LOGGER.info('\\n')\n","    LOGGER.info(f\"========== fold: {n_fold} training ==========\")\n","    train_loader, valid_loader = prepare_fold(fold=n_fold)\n","    LOGGER.info(f'Number of batches in Train {len(train_loader) } and valid {len(valid_loader)} dataset')\n","    model  = BaselineModel(cfg.model_name).to(cfg.device)   \n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=cfg.encoder_lr, \n","                                                decoder_lr=cfg.decoder_lr,\n","                                                weight_decay=cfg.weight_decay)\n","\n","    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n","    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.T_max, \n","                                                   eta_min=cfg.min_lr)    \n","    \n","   \n","    criterion = nn.SmoothL1Loss(reduction='mean')\n","    \n","    start = time.time()\n","    best_epoch_score = np.inf\n","    for epoch in range(cfg.num_epoch):\n","        \n","        train_loss  = train_run(model ,criterion ,optimizer , dataloader=train_loader)\n","        valid_loss , valid_preds , valid_labels, stuid_collect  = valid_run(model , dataloader=valid_loader)\n","        \n","        if valid_loss < best_epoch_score:\n","            \n","            LOGGER.info(f\"Validation Loss Improved ({best_epoch_score} ---> {valid_loss})\")\n","            best_epoch_score = valid_loss\n","            ### saving weights\n","            torch.save(model.state_dict(), f\"{cfg.only_model_name}_Fold_{n_fold}.pth\") \n","            \n","            ## saving oof values\n","            stuid_collect= [item for cell in stuid_collect for item in cell]\n","            df_ = oof_df(stuid_collect , valid_labels , valid_preds)\n","            \n","            LOGGER.info(f'Weights and oof values saved for epochs-{epoch} .....')\n","            \n","        LOGGER.info(f\"Epoch {epoch} Training Loss {np.round(train_loss , 4)} Validation Loss {np.round(valid_loss , 4)}\")\n","        \n","    end = time.time()\n","    time_elapsed = end - start\n","    \n","    LOGGER.info(' Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    \n","    LOGGER.info(\" Best Loss: {:.4f}\".format(best_epoch_score))\n","    \n","    oof_dfs.append(df_)            \n","    LOGGER.info(f\" oof for fold {n_fold} ---> {score_loss(valid_labels, valid_preds )}\")\n","    del model, train_loader, valid_loader , df_ , valid_preds , valid_labels\n","    gc.collect()\n","    LOGGER.info('\\n')"]},{"cell_type":"code","execution_count":33,"metadata":{"papermill":{"duration":0.084269,"end_time":"2023-08-05T14:55:06.325902","exception":false,"start_time":"2023-08-05T14:55:06.241633","status":"completed"},"tags":[]},"outputs":[],"source":["oof_df = pd.concat(oof_dfs , ignore_index=True )\n","oof_df.to_csv('oof_df.csv' , index = False)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>pred_content</th>\n","      <th>pred_wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3e7615c174b4</td>\n","      <td>-0.002466</td>\n","      <td>-0.045439</td>\n","      <td>0.694538</td>\n","      <td>0.606295</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9b8236dd1e52</td>\n","      <td>-0.861984</td>\n","      <td>-0.878322</td>\n","      <td>-1.044004</td>\n","      <td>-0.977210</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3ffce719094a</td>\n","      <td>-0.981265</td>\n","      <td>-1.548900</td>\n","      <td>-0.589357</td>\n","      <td>-0.752414</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>b0bc93ae93e2</td>\n","      <td>-0.602425</td>\n","      <td>-1.039843</td>\n","      <td>-0.772868</td>\n","      <td>-0.781131</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>c32175412e63</td>\n","      <td>-1.295922</td>\n","      <td>-0.620512</td>\n","      <td>-0.457843</td>\n","      <td>-0.495728</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id   content   wording  pred_content  pred_wording\n","0  3e7615c174b4 -0.002466 -0.045439      0.694538      0.606295\n","1  9b8236dd1e52 -0.861984 -0.878322     -1.044004     -0.977210\n","2  3ffce719094a -0.981265 -1.548900     -0.589357     -0.752414\n","3  b0bc93ae93e2 -0.602425 -1.039843     -0.772868     -0.781131\n","4  c32175412e63 -1.295922 -0.620512     -0.457843     -0.495728"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["oof_df.head()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["y_trues_final = np.array([oof_df[\"content\"].to_list(), oof_df['wording'].to_list()]).T\n","y_preds_final  = np.array([oof_df[\"pred_content\"].to_list(), oof_df[\"pred_wording\"].to_list()]).T"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(0.7651098238103811, [0.6903080109538932, 0.839911636666869])\n"]}],"source":["print(MCRMSE(y_trues_final, y_preds_final))"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>student_id</th>\n","      <th>prompt_id</th>\n","      <th>text</th>\n","      <th>content</th>\n","      <th>wording</th>\n","      <th>summary_length</th>\n","      <th>splling_err_num</th>\n","      <th>prompt_question</th>\n","      <th>prompt_title</th>\n","      <th>prompt_text</th>\n","      <th>prompt_length</th>\n","      <th>length_ratio</th>\n","      <th>word_overlap_count</th>\n","      <th>bigram_overlap_count</th>\n","      <th>trigram_overlap_count</th>\n","      <th>quotes_count</th>\n","      <th>fold</th>\n","      <th>pred_content</th>\n","      <th>pred_wording</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000e8c3c7ddb</td>\n","      <td>814d6b</td>\n","      <td>The third wave was an experimentto see how peo...</td>\n","      <td>0.205683</td>\n","      <td>0.380538</td>\n","      <td>69</td>\n","      <td>5</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.102832</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.0</td>\n","      <td>0.474785</td>\n","      <td>0.732196</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0020ae56ffbf</td>\n","      <td>ebad26</td>\n","      <td>They would rub it up with soda to make the sme...</td>\n","      <td>-0.548304</td>\n","      <td>0.506755</td>\n","      <td>56</td>\n","      <td>2</td>\n","      <td>Summarize the various ways the factory would u...</td>\n","      <td>Excerpt from The Jungle</td>\n","      <td>With one member trimming beef in a cannery, an...</td>\n","      <td>1137</td>\n","      <td>0.049252</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>-0.828328</td>\n","      <td>-0.217685</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004e978e639e</td>\n","      <td>3b9047</td>\n","      <td>In Egypt, there were many occupations and soci...</td>\n","      <td>3.128928</td>\n","      <td>4.231226</td>\n","      <td>285</td>\n","      <td>32</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.437788</td>\n","      <td>1</td>\n","      <td>56</td>\n","      <td>26</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>1.747052</td>\n","      <td>1.735659</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>005ab0199905</td>\n","      <td>3b9047</td>\n","      <td>The highest class was Pharaohs these people we...</td>\n","      <td>-0.210614</td>\n","      <td>-0.471415</td>\n","      <td>43</td>\n","      <td>5</td>\n","      <td>In complete sentences, summarize the structure...</td>\n","      <td>Egyptian Social Structure</td>\n","      <td>Egyptian society was structured like a pyramid...</td>\n","      <td>651</td>\n","      <td>0.066052</td>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.148915</td>\n","      <td>-0.067203</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0070c9e7af47</td>\n","      <td>814d6b</td>\n","      <td>The Third Wave developed  rapidly because the ...</td>\n","      <td>3.272894</td>\n","      <td>3.219757</td>\n","      <td>253</td>\n","      <td>29</td>\n","      <td>Summarize how the Third Wave developed over su...</td>\n","      <td>The Third Wave</td>\n","      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n","      <td>671</td>\n","      <td>0.377049</td>\n","      <td>1</td>\n","      <td>27</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>3.0</td>\n","      <td>1.502916</td>\n","      <td>1.255154</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     student_id prompt_id                                               text  \\\n","0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n","1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n","2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n","3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n","4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n","\n","    content   wording  summary_length  splling_err_num  \\\n","0  0.205683  0.380538              69                5   \n","1 -0.548304  0.506755              56                2   \n","2  3.128928  4.231226             285               32   \n","3 -0.210614 -0.471415              43                5   \n","4  3.272894  3.219757             253               29   \n","\n","                                     prompt_question  \\\n","0  Summarize how the Third Wave developed over su...   \n","1  Summarize the various ways the factory would u...   \n","2  In complete sentences, summarize the structure...   \n","3  In complete sentences, summarize the structure...   \n","4  Summarize how the Third Wave developed over su...   \n","\n","                prompt_title  \\\n","0             The Third Wave   \n","1    Excerpt from The Jungle   \n","2  Egyptian Social Structure   \n","3  Egyptian Social Structure   \n","4             The Third Wave   \n","\n","                                         prompt_text  prompt_length  \\\n","0  Background \\r\\nThe Third Wave experiment took ...            671   \n","1  With one member trimming beef in a cannery, an...           1137   \n","2  Egyptian society was structured like a pyramid...            651   \n","3  Egyptian society was structured like a pyramid...            651   \n","4  Background \\r\\nThe Third Wave experiment took ...            671   \n","\n","   length_ratio  word_overlap_count  bigram_overlap_count  \\\n","0      0.102832                   0                     5   \n","1      0.049252                   0                    22   \n","2      0.437788                   1                    56   \n","3      0.066052                   1                    10   \n","4      0.377049                   1                    27   \n","\n","   trigram_overlap_count  quotes_count  fold  pred_content  pred_wording  \n","0                      0             0   3.0      0.474785      0.732196  \n","1                     10             0   2.0     -0.828328     -0.217685  \n","2                     26             2   1.0      1.747052      1.735659  \n","3                      6             0   1.0      0.148915     -0.067203  \n","4                      5             4   3.0      1.502916      1.255154  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Merge data with original table\n","train_df_deberta = train_df.merge(oof_df.drop(['content', 'wording'], axis=1), on= \"student_id\")\n","train_df_deberta.head()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["targets = [\"content\", \"wording\"]\n","\n","drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n","                \"prompt_question\", \"prompt_title\", \n","                \"prompt_text\"\n","               ] + targets"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 1394\n","[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 10\n","[LightGBM] [Info] Start training from score 0.017606\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[45]\ttrain's rmse: 0.429272\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1368\n","[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 10\n","[LightGBM] [Info] Start training from score -0.039959\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[21]\ttrain's rmse: 0.671003\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1347\n","[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 10\n","[LightGBM] [Info] Start training from score 0.013356\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[66]\ttrain's rmse: 0.470571\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1401\n","[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 10\n","[LightGBM] [Info] Start training from score -0.044904\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[40]\ttrain's rmse: 0.736404\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1394\n","[LightGBM] [Info] Number of data points in the train set: 5108, number of used features: 10\n","[LightGBM] [Info] Start training from score -0.031791\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[34]\ttrain's rmse: 0.627097\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1368\n","[LightGBM] [Info] Number of data points in the train set: 5156, number of used features: 10\n","[LightGBM] [Info] Start training from score -0.060941\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[6]\ttrain's rmse: 0.852976\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1347\n","[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 10\n","[LightGBM] [Info] Start training from score 0.028040\n","Training until validation scores don't improve for 30 rounds\n","Early stopping, best iteration is:\n","[52]\ttrain's rmse: 0.601424\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1401\n","[LightGBM] [Info] Number of data points in the train set: 6062, number of used features: 10\n","[LightGBM] [Info] Start training from score -0.168933\n","Training until validation scores don't improve for 30 rounds\n","[100]\ttrain's rmse: 1.11055\n","Early stopping, best iteration is:\n","[74]\ttrain's rmse: 1.10306\n"]}],"source":["import lightgbm as lgb\n","import pickle\n","\n","model_dict = {}\n","\n","for target in targets:\n","    models = []\n","    \n","    for fold in range(cfg.fold):\n","\n","        X_train_cv = train_df_deberta[train_df_deberta[\"fold\"] != fold].drop(columns=drop_columns)\n","        y_train_cv = train_df_deberta[train_df_deberta[\"fold\"] != fold][target]\n","\n","        X_eval_cv = train_df_deberta[train_df_deberta[\"fold\"] == fold].drop(columns=drop_columns)\n","        y_eval_cv = train_df_deberta[train_df_deberta[\"fold\"] == fold][target]\n","\n","        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n","        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n","\n","        params = {\n","                  'boosting_type': 'gbdt',\n","                  'random_state': 42,\n","                  'objective': 'regression',\n","                  'metric': 'rmse',\n","                  'learning_rate': 0.05,\n","                  }\n","\n","        evaluation_results = {}\n","        model = lgb.train(params,\n","                          num_boost_round=10000,\n","                            #categorical_feature = categorical_features,\n","                          valid_names=['train', 'valid'],\n","                          train_set=dtrain,\n","                          valid_sets=dval,\n","                          callbacks=[\n","                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n","                               lgb.log_evaluation(100),\n","                              lgb.callback.record_evaluation(evaluation_results)\n","                            ],\n","                          )\n","        models.append(model)\n","    \n","    model_dict[target] = models\n","\n","pickle.dump(model_dict, open('lgb_models.pkl', 'wb'))\n","print('Trained LGB model was saved!')"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["# model_dict_load = pickle.load(open('lgb_models.pkl', 'rb'))"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["content_rmse : 0.569488377734257\n","wording_rmse : 0.7778014596795643\n","mcrmse : 0.6736449187069107\n"]}],"source":["rmses = []\n","\n","for target in targets:\n","    models = model_dict[target]\n","\n","    preds = []\n","    trues = []\n","    \n","    for fold, model in enumerate(models):\n","        # ilocで取り出す行を指定\n","        X_eval_cv = train_df_deberta[train_df_deberta[\"fold\"] == fold].drop(columns=drop_columns)\n","        y_eval_cv = train_df_deberta[train_df_deberta[\"fold\"] == fold][target]\n","\n","        pred = model.predict(X_eval_cv)\n","\n","        trues.extend(y_eval_cv)\n","        preds.extend(pred)\n","        \n","    rmse = np.sqrt(mean_squared_error(trues, preds))\n","    print(f\"{target}_rmse : {rmse}\")\n","    rmses = rmses + [rmse]\n","\n","print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
